<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <link rel="icon" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/favicon.ico" />
    <!-- Preload is necessary because we show these images when we disconnect from the server,
    but at that point we cannot load these images from the server -->
    <link rel="preload" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/gradient-yHQUC_QB.png" as="image" />
    <link rel="preload" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/noise-60BoTA8O.png" as="image" />
    <!-- Preload the fonts -->
    <link rel="preload" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/Lora-VariableFont_wght-B2ootaw-.ttf" as="font" crossorigin="anonymous" />
    <link rel="preload" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/PTSans-Regular-CxL0S8W7.ttf" as="font" crossorigin="anonymous" />
    <link rel="preload" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/PTSans-Bold-D9fedIX3.ttf" as="font" crossorigin="anonymous" />
    <link rel="preload" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/FiraMono-Regular-BTCkDNvf.ttf" as="font" crossorigin="anonymous" />
    <link rel="preload" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/FiraMono-Medium-DU3aDxX5.ttf" as="font" crossorigin="anonymous" />
    <link rel="preload" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/FiraMono-Bold-CLVRCuM9.ttf" as="font" crossorigin="anonymous" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="theme-color" content="#000000" />
    <meta name="description" content="a marimo app" />
    <link rel="apple-touch-icon" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/apple-touch-icon.png" />
    <link rel="manifest" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/manifest.json" />

    <script data-marimo="true">
      function __resizeIframe(obj) {
        var scrollbarHeight = 20; // Max between windows, mac, and linux

        function setHeight() {
          // Guard against race condition where iframe isn't ready
          if (!obj.contentWindow?.document?.documentElement) {
            return;
          }
          var element = obj.contentWindow.document.documentElement;
          // If there is no vertical scrollbar, we don't need to resize the iframe
          if (element.scrollHeight === element.clientHeight) {
            return;
          }

          // Create a new height that includes the scrollbar height if it's visible
          var hasHorizontalScrollbar = element.scrollWidth > element.clientWidth;
          var newHeight = element.scrollHeight + (hasHorizontalScrollbar ? scrollbarHeight : 0);

          // Only update the height if it's different from the current height
          if (obj.style.height !== `${newHeight}px`) {
            obj.style.height = `${newHeight}px`;
          }
        }

        // Resize the iframe to the height of the content and bottom scrollbar height
        setHeight();

        // Resize the iframe when the content changes
        const resizeObserver = new ResizeObserver((entries) => {
          setHeight();
        });
        // Only observe if iframe content is ready
        if (obj.contentWindow?.document?.body) {
          resizeObserver.observe(obj.contentWindow.document.body);
        }
      }
    </script>
    <marimo-filename hidden>ReadDigitsTraining.py</marimo-filename>
    <!-- TODO(Trevor): Legacy, required by VS Code plugin. Remove when plugin is updated (see marimo/server/_templates/template.py) -->
    <marimo-version data-version="{{ version }}" hidden></marimo-version>
    <marimo-user-config data-config="{{ user_config }}" hidden></marimo-user-config>
    <marimo-server-token data-token="{{ server_token }}" hidden></marimo-server-token>
    <!-- /TODO -->
    <title>MNIST Digits</title>
    <script type="module" crossorigin crossorigin="anonymous" src="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/index-BXqkmYCF.js"></script>
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/preload-helper-BnutJmlU.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/chunk-DZLz74EQ.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/react-BcIddLXZ.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/compiler-runtime-Cr9loedd.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/jsx-runtime-mwDPpfh_.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/useEventListener-D1FLsfmf.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/react-dom-D6bRRS5R.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/Combination-BjaZc6gK.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/clsx-BlAkgCmw.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/cn-ZXtYmczU.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/menu-items-DwOFR2Zi.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/hotkeys-ClCtr3_I.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/button-ZC2axIBq.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/dist-o-4-WxUj.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/createLucideIcon-CNVopWor.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/check-CuBLr6Cm.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/select-5fcaZwou.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/multi-map-D9JaQ_m6.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/chevron-right-BV0VfNbL.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/dropdown-menu-uN_XMhZm.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/tooltip-BHnVWzH0.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/_Uint8Array-D5Z9rM2X.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/_getTag-CVRrQL_Q.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/_baseIsEqual-CVFfFJm2.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/jotai-BTnrNcC1.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/invariant-D5a0EE05.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/isArrayLikeObject-DKyJYtr8.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/merge-Dvc5opZF.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/zod-U2NFpcFA.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/utils-DEDX0RLo.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/requests-BACZP_uj.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/once-C5bmD4W5.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/toDate-C3oOx2yr.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/_baseSlice-BWZ0PeaJ.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/_arrayMap-DQI2GUNb.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/isSymbol-xTSywenE.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/toString-CBnct2wx.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/_hasUnicode-3DIK_jNi.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/upperFirst-CBQfDRod.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/capitalize-CBqvgOfA.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/ai-model-dropdown-CvhbTHha.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/badge-Jfhpc30N.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/use-toast-midBZAtS.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/constants-CEISdYm1.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/Deferred-NfAvvkGM.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/config-C6qInAhC.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/uuid-D0hUC3Qw.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/key-Cx1qY8eM.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/connection-Bs4SQ_vu.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/useLifecycle-DogGOXPh.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/useNonce-VIoWEX5z.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/capabilities-srilPYuF.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/createReducer-CeLhipaI.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/dist-MHIqsPCF.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/dist-8_YHxBVW.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/dist-CbvGwhr0.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/dist-CPUmJw_F.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/dist-DcDuMWp5.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/dist-DJV2CjUA.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/dist-C14Sk-Bd.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/dist-vBG1bGij.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/dist-Oad1venD.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/stex-xveX5l_T.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/cjs-Ck8KMDI1.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/_baseProperty-c4IQJQis.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/toNumber-CTOPJrpu.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/now-BXqTLlmI.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/debounce-BUM2losY.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/toInteger-Bor4StOs.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/type-o43RKOcp.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/marked.esm-BV7g2d65.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/main-ChjX94eX.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/cells-A69p56PE.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/kbd-DbVT085u.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/renderShortcut-DJmsjON-.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/useEvent-CRDKqG3C.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/useDeleteCell-CEsjHDdl.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/lazy-oPCv0nK8.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/defaultLocale-YmL2k7Vp.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/precisionRound-Qxa8hC2h.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/defaultLocale-zQltfbSS.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/vega-loader.browser-D2m26-DP.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/tooltip-BFMRXuzR.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/ErrorBoundary-_uEOjNWh.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/alert-dialog-8EwWlboa.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/dialog-BfhgTWm1.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/useDebounce-DzG_89QJ.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/numbers-CTSqyVyD.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/SSRProvider-B8x7qo1c.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/context-CcLKJeA2.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/useNumberFormatter-F8R3LBxf.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/usePress-BO20vkQb.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/input-BNb8K99m.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/ImperativeModal-C4fZHLKz.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/cell-link-D75IV6D1.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/alert-BqjCH_7t.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/links-CMl0qG9J.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/state-JZFfOGr9.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/MarimoErrorOutput-BATnwNYX.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/spinner-CuquyDWA.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/_arrayReduce-DV4IVFuG.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/startCase-RfubwhHM.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/strings-DHsDEFo-.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/command-Dlk4i3l7.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/popover-F0Sfc1dn.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/useRunCells-BLpH_l8E.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/copy-LsxoSTiV.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/copy-Dy_tLkL8.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/copy-icon-CE7wmMnS.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/mode-Bq6LjPC-.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/purify.es-CPMOFAIu.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/RenderHTML-DTLkV9Uh.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/table-DU76OrPF.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/tabs-l9npw3ob.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/useAsyncData-CngUoIwV.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/useIframeCapabilities-DXQ78nsN.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/error-banner-DoSAS3iD.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/formats-BN93kMCb.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/useTheme-Biy2QI0v.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/en-US-D2gH55FJ.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/isValid-BJSfXqfd.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/maps-B1z3jt-a.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/shim-Cc3VMan6.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/download-ByFIcaoi.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/extends-dwNt6GDg.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/emotion-is-prop-valid.esm-OoyWz_0l.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/useDateFormatter-CPDTrKqK.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/range-ZclXTdAh.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/table-C2gUjtJV.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/Output-DWmT-7NV.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/name-cell-input-DND1MD7b.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/useHotkey-NDxGNWnW.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/objectWithoutPropertiesLoose-COpOcp6W.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/esm-B3nKz9z-.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/dist-GE2AwHvG.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/dist-CMHCymvx.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/dist-ta4iJSan.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/dist-BcppZLnk.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/dist-DRTi9ht4.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/dist-BjGT2Oog.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/dist-CXL8T4lt.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/dist-B0B_Xnwd.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/dist-BcgNLefj.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/dist-j2I9I0a7.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/dist-DM2w8f7r.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/dist-nhBdVy6b.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/dist-D-JTXier.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/apl-BGS-6pM9.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/asciiarmor-CbQH35y7.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/asn1-P7HrFGaM.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/brainfuck-CSxeyQc4.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/clike-Bg9duJDo.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/clojure-B94DOmFw.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/cmake-DQEKyMNG.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/cobol-CY4gVoPW.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/coffeescript-BwWb96Ts.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/commonlisp-Cq_n1sF6.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/crystal-BLTq7Sqq.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/css-CmDcdojQ.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/cypher-ByytCYVx.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/d-xLFRg71F.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/diff-VF6GbDch.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/dtd-D8qb8bJW.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/dylan-CIkom9X4.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/ecl-8wcmTiom.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/eiffel-D2ZBcT03.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/elm-Cpg3TnMc.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/erlang-B4A2pXCG.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/simple-mode-BrL6eoj-.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/factor-DhbRkYZ5.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/forth-C5F1F4pC.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/fortran-D7Gl3Tli.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/gas-BS2f8qHc.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/gherkin-CVZcskwU.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/groovy-BhQsBMwW.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/haskell-BPbxlKgK.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/haxe-TKSAmiMf.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/idl-DFgQu5Ek.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/javascript-dAA8UGkz.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/jinja2-BIbcpWyF.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/julia-CfdFJG3K.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/livescript-FcXY3DYa.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/lua-Co6rjiMf.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/mathematica-yb42KhnJ.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/mbox-Kdy3h0JS.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/mirc-DIfBuSAc.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/mllike-ClozcVVR.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/modelica-DfSVZeoi.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/mscgen-BCcB4vmY.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/mumps-Zs7zFruB.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/nsis-znELVT3h.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/ntriples-BZ-Gx5MV.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/octave-DbhnFGQl.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/oz-C8cfEyhB.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/pascal-DjQLfZxE.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/perl-BbHwsgL1.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/pig-siFNrhhJ.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/powershell-DtGrdgtc.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/properties-ThqBcpfC.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/protobuf-RiYv9VfD.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/pug-DUVdXXqU.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/puppet-U5CQLRYv.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/python-Bwt6zAaD.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/q-DIhdXtBp.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/r-CWawXx6U.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/rpm-CNr8ALvx.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/ruby-DskfS84U.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/sas-C2b2tSOH.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/scheme-Cmmcrp64.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/shell-Dyv1aCg9.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/sieve-B_dGzCIS.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/smalltalk-intFzbz6.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/sparql-BX1c9HJU.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/stylus-X6xt1FMB.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/swift-BYNJXGdg.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/tcl-BaybuQuE.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/textile-77KFAyfi.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/toml-D6E8t1IG.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/troff-BfktZMhT.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/ttcn-cfg-CstA0KVC.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/ttcn-DOlRqUFL.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/turtle-Cx-tHjh2.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/vb-DerFRxqu.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/vbscript-BPEY3B7J.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/velocity-B-lFP8Gk.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/verilog-C9TzcLDl.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/vhdl-v8a_uH2H.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/webidl-DmZw40AA.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/xquery-BrIWpdRx.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/yacas-ofPECLUx.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/z80-DrGVKC-O.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/esm-DtCFmQ1n.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/trash-CRcefzxk.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/ConsoleOutput-BJP_UIwR.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/icons-DaFojEUL.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/add-missing-import-Bc1Nj8aC.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/blob-ISoY06EG.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/add-cell-with-ai-Bf3hYYIu.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/chart-no-axes-column-DYZiSI6e.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/square-function-sjebOKTF.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/spec-C5SzKl5N.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/focus-BaSDCTOl.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/useInstallPackage-I3kefbOO.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/column-preview-WPlkULp9.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/useAddCell-CREgcbTt.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/toggle-DlfY8wql.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/api-YJHlT7KM.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/globals-Dp5TPAWC.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/share-8z32Sr0F.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/dist-DBrqGLzW.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/memoize-DhF8x4Aa.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/_toKey-16owuO_q.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/_baseSet-L2qemYhz.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/utilities.esm-Bjl8-mRM.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/floating-outline-1_-JOoBX.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/eye-off-_Bp7Ir5E.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/plus-CRGccegF.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/readonly-python-code-B4MS39iL.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/file-video-camera-DJUITpag.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/file-text-CFgIi1A4.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/file-Cz8xVTWu.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/types-Bh2kSb2q.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/label-Dcu3RhQ4.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/form-CaSJmn0I.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/textarea-DeG5mMSH.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/circle-x-Dm9uA-wT.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/refresh-ccw-Dg4gNnPm.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/trash-2-BPux4_UF.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/form-DaO98mWt.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/field-CaHPDBhq.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/switch-4s4187gV.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/useBoolean-CpaqzlKu.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/useDeepCompareMemoize-DiJVYA6I.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/LazyAnyLanguageCodeMirror-BovGLUi6.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/types-CHAkS4Oz.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/prop-types-Dr_68tMt.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/es-vLBD1nBl.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/VisuallyHidden-DqOWapbP.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/_baseFlatten-ViL-mfkw.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/get-BUGWIXZE.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/hasIn-BgJKauE7.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/_basePickBy-CSDa7ReR.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/pick-BQzWKfma.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/square-CYRwTwZO.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/download-ptuQ0M1w.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/globe-mi3m-i9B.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/send-CTRovYct.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/refresh-cw-Duwwh0cP.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/settings-p8vTgdrN.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/bundle.esm-BEDtDq85.js">
    <link rel="stylesheet" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/cells-BvrC6Bsv.css">
    <link rel="stylesheet" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/Output-j3zW187k.css">
    <link rel="stylesheet" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/ConsoleOutput-BFFTcC5Y.css">
    <link rel="stylesheet" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.17.7/dist/assets/index-BmcO-QYJ.css">
  
<script data-marimo="true">
    window.__MARIMO_STATIC__ = {};
    window.__MARIMO_STATIC__.files = {};
</script>
</head>
  <body>
    <div id="root"></div>
    <!-- This is a portal for the data editor to render in -->
    <div id="portal" data-testid="glide-portal" style="position: fixed; left: 0; top: 0; z-index: 9999"></div>
    <script data-marimo="true">
      window.__MARIMO_MOUNT_CONFIG__ = {
            "filename": "ReadDigitsTraining.py",
            "mode": "read",
            "version": "0.17.7",
            "serverToken": "static",
            "config": {"ai": {"models": {"custom_models": [], "displayed_models": []}}, "completion": {"activate_on_typing": true, "copilot": false}, "diagnostics": {"sql_linter": true}, "display": {"cell_output": "above", "code_editor_font_size": 14, "custom_css": [], "dataframes": "rich", "default_table_max_columns": 50, "default_table_page_size": 10, "default_width": "full", "reference_highlighting": false, "theme": "light"}, "formatting": {"line_length": 79}, "keymap": {"overrides": {}, "preset": "default"}, "language_servers": {"pylsp": {"enable_flake8": false, "enable_mypy": true, "enable_pydocstyle": false, "enable_pyflakes": false, "enable_pylint": false, "enable_ruff": true, "enabled": false}}, "mcp": {"mcpServers": {}, "presets": []}, "package_management": {"manager": "uv"}, "runtime": {"auto_instantiate": false, "auto_reload": "off", "default_sql_output": "auto", "on_cell_change": "autorun", "output_max_bytes": 8000000, "reactive_tests": true, "std_stream_max_bytes": 1000000, "watcher_on_save": "lazy"}, "save": {"autosave": "after_delay", "autosave_delay": 1000, "format_on_save": false}, "server": {"browser": "default", "follow_symlink": false}, "snippets": {"custom_paths": [], "include_default_snippets": true}},
            "configOverrides": {},
            "appConfig": {"app_title": "MNIST Digits", "sql_output": "auto", "width": "full"},
            "view": {"showAppCode": true},
            "notebook": {"cells": [{"code": "mo.md(r\"\"\"\n# MNIST Digits\n\nIn this notebook we are going to train a neural network to recognize handwritten digits. This is the  \"Hello World\" of deep learning: training a deep learning model to correctly classify hand-written digits.\n\nIn the previous [notebook](TheMNISTDataSet.ipynb) we downloaded the MNIST dataset, which is a dataset of 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images.  We will re-use this data (downloaded either to your local hard drive or /transfer) to train a neural network to recognize the digits.\n\nWe will start by importing the necessary libraries, including our Utils module.\n\"\"\")", "code_hash": "66f31c9256cd66a3c4b20df893c26e5f", "config": {"column": null, "disabled": false, "hide_code": true}, "id": "Hbol", "name": "_"}, {"code": "import mlutils\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport struct\nimport sys\nimport torch\nimport torch.nn as nn\nfrom torch.optim import Adam\nfrom torch.utils.data import Dataset, DataLoader\n\n# Visualization tools\nimport torchvision.transforms.v2 as transforms\n\nprint(f\"{mlutils.in_lab()=}\")", "code_hash": "d764246e87a6a92079a807799703ffbd", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "MJUe", "name": "_"}, {"code": "mo.md(r\"\"\"\n## GPU Support\n\nIn this notebook we will use the GPU to train our model, we can use the function from our Utils module to check if the GPU is available and set this as the device to use for our data.\n\"\"\")", "code_hash": "f209ec4d4d52fc6d52e5bc9028e3c30d", "config": {"column": null, "disabled": false, "hide_code": true}, "id": "vblA", "name": "_"}, {"code": "device = mlutils.get_device()\nprint(device)", "code_hash": "84beed4487e172d5a3c6e183007ba805", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "bkHC", "name": "_"}, {"code": "mo.md(r\"\"\"\n## Image classification\n\nThe approach we are going to take with this example is to load a set of know images and their labels, and train a neural network to learn the relationship between the images and their labels.\n\nWe will use a neural network and a trial and error system to begin to recognize the patterns in the images. The images are small (28x28 pixels) and the neural network will learn to recognize the patterns in the images that are associated with the digits.\n\nWe have a set of 60,000 images to train the network and a separate set of 10,000 images to test the network and on each step of the training process we will check the accuracy of the network on the test set.\n\nIn the previous notebook we created two functions for loading the data and labels so we will use these functions to load the data and labels for the training and test sets.\n\"\"\")", "code_hash": "1b053b19b189db9368b267b982ff8f2a", "config": {"column": null, "disabled": false, "hide_code": true}, "id": "lEQa", "name": "_"}, {"code": "def load_mnist_labels(filename: str) -\u003E np.ndarray:\n    with open(filename, \"rb\") as f:\n        magic, num = struct.unpack(\"\u003EII\", f.read(8))\n        labels = np.fromfile(f, dtype=np.uint8)\n        if len(labels) != num:\n            raise ValueError(f\"Expected {num} labels, but got {len(labels)}\")\n    return labels\n\n\ndef load_mnist_images(filename: str) -\u003E np.ndarray:\n    with open(filename, \"rb\") as f:\n        magic, num, rows, cols = struct.unpack(\"\u003EIIII\", f.read(16))\n        images = np.fromfile(f, dtype=np.uint8).reshape(num, rows, cols)\n        if len(images) != num:\n            raise ValueError(f\"Expected {num} images, but got {len(images)}\")\n    return images", "code_hash": "e76b3765a5811dd1d45ebcefac073255", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "PKri", "name": "_"}, {"code": "DATASET_LOCATION = \"\"\nif mlutils.in_lab():\n    DATASET_LOCATION = \"/transfer/MNIST_Dataset/\"\nelse:\n    DATASET_LOCATION = \"./MNIST_Dataset/\"\n\ntrain_labels = load_mnist_labels(DATASET_LOCATION + \"train-labels-idx1-ubyte\")\ntest_labels = load_mnist_labels(DATASET_LOCATION + \"t10k-labels-idx1-ubyte\")\n\nprint(len(train_labels), len(test_labels))\nprint(train_labels[0], test_labels[0])\n\n# We can now load the images from both the datasets.\ntrain_images = load_mnist_images(DATASET_LOCATION + \"train-images-idx3-ubyte\")\ntest_images = load_mnist_images(DATASET_LOCATION + \"t10k-images-idx3-ubyte\")", "code_hash": "793637d94bd27aded8091af070822a5f", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "Xref", "name": "_"}, {"code": "mo.md(r\"\"\"\nTo see the images we can define a simple function to display the images. We will use the matplotlib library to display the images.\n\"\"\")", "code_hash": "5dce7eb5bb282fedb2d6754b5735bb32", "config": {"column": null, "disabled": false, "hide_code": true}, "id": "SFPL", "name": "_"}, {"code": "def display_image(image: np.array, label: str) -\u003E None:\n    plt.figure(figsize=(1, 1))\n    plt.title(f\"Label : {label}\")\n    plt.imshow(image, cmap=\"gray\")\n    plt.axis(\"off\")\n    plt.show()\n\n\n# We can now display the first image from the training dataset.\n\ndisplay_image(train_images[0], train_labels[0])\nprint(type(train_images[0]))\nprint(train_images[0].shape)\nprint(train_images[0].dtype)", "code_hash": "970c1edcfdeeebdc09fec1b62c50dbdd", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "BYtC", "name": "_"}, {"code": "mo.md(r\"\"\"\nIf we look at the data it is stored in a numpy array of 28,28 and a single unsigned char data type. We need to transform this data into the correct type for machine learning. In particular we need to convert the data into a Tensor of type float32, then we need to batch the data into a DataLoader.\n\nWe can use the torchvision library to transform our data as follows.\n\"\"\")", "code_hash": "ba5fa201bb5e36c8c372a6f7d4f832d6", "config": {"column": null, "disabled": false, "hide_code": true}, "id": "RGSE", "name": "_"}, {"code": "trans = transforms.Compose(\n    [transforms.ToImage(), transforms.ToDtype(torch.float32, scale=True)]\n)\ntensor = trans(train_images[0])\nprint(tensor.shape)\nprint(tensor.dtype)\nprint(tensor.min(), tensor.max())\nprint(tensor.device)", "code_hash": "6cd5fe55c9d18b15f04b2e1589e59979", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "Kclp", "name": "_"}, {"code": "mo.md(r\"\"\"\nBy default the data for this tensor is processed on the CPU, we can convert it to run on the GPU by using the .to(device) method.\n\"\"\")", "code_hash": "9299a2c768f59430b1aa6205f3fe561b", "config": {"column": null, "disabled": false, "hide_code": true}, "id": "emfo", "name": "_"}, {"code": "tensor.to(device).device", "code_hash": "480b1eaa889d09b0c84dfae4104e39d6", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "Hstk", "name": "_"}, {"code": "mo.md(r\"\"\"\n## Preparing the Data for Training\n\nEarlier, we created a `trans` variable to convert our ndarray to a tensor. [Transforms](https://pytorch.org/vision/stable/transforms.html) are a group of torchvision functions that can be used to transform a dataset.\n\nAt present our train_images and test_images are numpy arrays. We need to convert them to tensors. We can do this using the `trans` variable we created earlier.\n\"\"\")", "code_hash": "91138c30b5768e2d22c9f9a748eb4899", "config": {"column": null, "disabled": false, "hide_code": true}, "id": "nWHF", "name": "_"}, {"code": "train_images_tensor = torch.tensor(train_images, dtype=torch.float32).to(\n    device\n)\ntrain_labels_tensor = torch.tensor(train_labels, dtype=torch.uint8).to(device)\n\ntest_images_tensor = torch.tensor(test_images, dtype=torch.float32).to(device)\ntest_labels_tensor = torch.tensor(test_labels, dtype=torch.uint8).to(device)", "code_hash": "6aca7f40bdd762e929f5e74d5e3fbea5", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "iLit", "name": "_"}, {"code": "mo.md(r\"\"\"\n## Dataloaders\n\nWe can use the DataLoader class from the torch.utils.data module to create a DataLoader for our training and test data, you can think of this as batching images into smaller groups for training.\n\nFirst we need to create a custom class to hold our data, we can do this by creating a subclass of the Dataset class from the torch.utils.data module. We need to implement the __len__ and __getitem__ methods to return the length of the dataset and the data and label for a given index.\n\"\"\")", "code_hash": "e54109fe61a308c068b9ba208d458d3b", "config": {"column": null, "disabled": false, "hide_code": true}, "id": "ZHCJ", "name": "_"}, {"code": "# Custom dataset class\nclass DigitsDataset(Dataset):\n    def __init__(self, images_tensor, labels_tensor):\n        self.images_tensor = images_tensor\n        self.labels_tensor = labels_tensor\n\n    def __len__(self):\n        return len(self.labels_tensor)\n\n    def __getitem__(self, idx):\n        image = self.images_tensor[idx]\n        label = self.labels_tensor[idx]\n        return image, label", "code_hash": "94bfd9930f48269b8245c422f80d8fd1", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "ROlb", "name": "_"}, {"code": "mo.md(r\"\"\"\nWe could show our models the entire dataset at once. Not only does this take a lot of computational resources, but [research shows](https://arxiv.org/pdf/1804.07612) using a smaller batch of data is more efficient for model training.\n\nFor example, if our `batch_size` is 32, we will train our model by shuffling the deck and drawing 32 cards. We do not need to shuffle for validation as the model is not learning, but we will still use a `batch_size` to prevent memory errors.\n\nThe batch size is something the model developer decides, and the best value will depend on the problem being solved. Research shows 32 or 64 is sufficient for many machine learning problems and is the default in some machine learning frameworks, so we will use 32 here.\n\"\"\")", "code_hash": "c3af0c0d266d8ee17dd2f3883c4986a3", "config": {"column": null, "disabled": false, "hide_code": true}, "id": "qnkX", "name": "_"}, {"code": "batch_size = 32\n\ntrain_data = DigitsDataset(train_images_tensor, train_labels_tensor)\nvalid_data = DigitsDataset(test_images_tensor, test_labels_tensor)\n\ntrain_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\nvalid_loader = DataLoader(valid_data, batch_size=batch_size)", "code_hash": "01d502382d079391ced9941a2cacdf85", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "TqIu", "name": "_"}, {"code": "mo.md(r\"\"\"\n## Creating a Model\n\nNeural networks are composed of layers where each layer performs a mathematical operation on the data it receives before passing it to the next layer. To start, we will create a \"Hello World\" level model made from 4 components:\n\n1. A [Flatten](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html) used to convert n-dimensional data into a vector.\n2. An input layer, the first layer of neurons\n3. A hidden layer, another layer of neurons \"hidden\" between the input and output\n4. An output layer, the last set of neurons which returns the final prediction from the model\n\nWe will use a variable called layers to store the layers of our model.\n\"\"\")", "code_hash": "521fdc426713fac3c7cc0eb6e889b491", "config": {"column": null, "disabled": false, "hide_code": true}, "id": "Vxnm", "name": "_"}, {"code": "mo.md(r\"\"\"\n## Flatten the image\n\nThe first thing we need to do is convert the image from a 28x28  array into a flat 1d tensor. We saw the images had 3 dimensions: `C x H x W`. To flatten an image means to combine all of these images into 1 dimension. Let's say we have a tensor like the one below.\n\"\"\")", "code_hash": "427c3c186ab9e25f36752a2be24309eb", "config": {"column": null, "disabled": false, "hide_code": true}, "id": "DnEU", "name": "_"}, {"code": "test_matrix = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nprint(test_matrix)\nprint(nn.Flatten()(test_matrix))", "code_hash": "1a326d870c3dd07a1145e88fbd87d929", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "ulZA", "name": "_"}, {"code": "mo.md(r\"\"\"\nYou will notice nothing happened, this is because neural networks expect to receive a batch of data. Currently, the Flatten layer sees three vectors as opposed to one 2d matrix. To fix this, we can \"batch\" our data by adding an extra pair of brackets. Since `test_matrix` is now a tensor, we can do that with the shorthand below. `None` adds a new dimension where `:` selects all the data in a tensor.\n\"\"\")", "code_hash": "99e75a78359e68b5f496ec78e2af2d3e", "config": {"column": null, "disabled": false, "hide_code": true}, "id": "ecfG", "name": "_"}, {"code": "batch_test_matrix = test_matrix[None, :]\nprint(batch_test_matrix)\nprint(nn.Flatten()(batch_test_matrix))", "code_hash": "6a2b7a5792f1a9ef29d1a585765ca0f4", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "Pvdt", "name": "_"}, {"code": "mo.md(r\"\"\"\n## The Input Layer\n\nThe input layer is the first layer of neurons in the neural network. It is responsible for receiving the input data and passing it to the next layer.\n\nThis layer will be *densely connected*, meaning that each neuron in it, and its weights, will affect every neuron in the next layer.\n\nIn order to create these weights, Pytorch needs to know the size of our inputs and how many neurons we want to create.\nSince we've flattened our images, the size of our inputs is the number of channels, number of pixels vertically, and number of pixels horizontally multiplied together.\n\"\"\")", "code_hash": "d969f5841c1b0d216cd8ab3e70cce476", "config": {"column": null, "disabled": false, "hide_code": true}, "id": "ZBYS", "name": "_"}, {"code": "input_size = 1 * 28 * 28\nprint(input_size)", "code_hash": "8a6b26fa192c199e71842549713fa50f", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "aLJB", "name": "_"}, {"code": "mo.md(r\"\"\"\nChoosing the correct number of neurons is what puts the \"science\" in \"data science\" as it is a matter of capturing the statistical complexity of the dataset. For now, we will use `512` neurons. Try playing around with this value later to see how it affects training and to start developing a sense for what this number means.\n\nWe will learn more about activation functions later, but for now, we will use the [relu](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) activation function, which in short, will help our network to learn how to make more sophisticated guesses about data than if it were required to make guesses based on some strictly linear function.\n\"\"\")", "code_hash": "984af51c3bd17946b5c3536e59c29cc5", "config": {"column": null, "disabled": false, "hide_code": true}, "id": "nHfw", "name": "_"}, {"code": "layers_1 = [nn.Flatten(), nn.Linear(input_size, 512), nn.ReLU()]\nlayers_1", "code_hash": "51a69ee8bf0787cc4b7a139750b9d43a", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "xXTn", "name": "_"}, {"code": "mo.md(r\"\"\"\n## The hidden layer\n\nA hidden layer is a layer of neurons between the input and output layers. It is called \"hidden\" because it is not directly exposed to the input data and the output predictions. We will cover why we combine multiple layers in another lecture, but for now, we will add a hidden layer to our model.\n\nAs with the previous layers, the shape of the data is important. [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) needs to know the shape of the data being passed to it. Each neuron in the previous layer will compute one number, so the number of inputs into the hidden layer is the same as the number of neurons in the previous later.\n\"\"\")", "code_hash": "1393beeec9f4088acc139841199371c4", "config": {"column": null, "disabled": false, "hide_code": true}, "id": "AjVT", "name": "_"}, {"code": "layers_2 = [\n    nn.Flatten(),\n    nn.Linear(input_size, 512),\n    nn.ReLU(),\n    nn.Linear(512, 512),\n    nn.ReLU(),\n]\nlayers_2", "code_hash": "ddabf47c974f5d80bb33c57e0257e71d", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "pHFh", "name": "_"}, {"code": "mo.md(r\"\"\"\n## The Output Layer\n\nThe output layer is the final layer of neurons in the neural network. It is responsible for producing the output of the model. This will be a tensor of length 10, where each element represents the probability of the input image being a particular digit.\n\nWe will not assign the `relu` function to the output layer. Instead, we will apply a `loss function` covered in the next section.\n\"\"\")", "code_hash": "835c457b3848f1e0325f140c7c8d6bdb", "config": {"column": null, "disabled": false, "hide_code": true}, "id": "NCOB", "name": "_"}, {"code": "n_classes = 10\nlayers_3 = [\n    nn.Flatten(),\n    nn.Linear(input_size, 512),\n    nn.ReLU(),\n    nn.Linear(512, 512),\n    nn.ReLU(),\n    nn.Linear(512, n_classes),\n]\nlayers_3", "code_hash": "5e0ba0758cb87a84acd07bbcac7b4a3a", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "aqbW", "name": "_"}, {"code": "mo.md(r\"\"\"\n## Compiling the Model\n\nA [Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) model expects a sequence of arguments, not a list, so we can use the [* operator](https://docs.python.org/3/reference/expressions.html#expression-lists) to unpack our list of layers into a sequence. We can print the model to verify these layers loaded correctly. We can also send our model to the GPU using the to(device) method. We can verify where the model is by using the .device attribute.\n\"\"\")", "code_hash": "e16ad0accc14568fb98956b12895ec23", "config": {"column": null, "disabled": false, "hide_code": true}, "id": "TRpd", "name": "_"}, {"code": "model = nn.Sequential(*layers_3)\nmodel.to(device)\nnext(model.parameters()).device", "code_hash": "37b9c250c9b9139e2b94d83d66b45a6f", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "TXez", "name": "_"}, {"code": "mo.md(r\"\"\"\nPyTorch 2.0 introduced the ability to [compile]([here](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html).) the model using the `compile` method which can give faster performance.\n\"\"\")", "code_hash": "8f11f663c9d49dbd7b168c892e75e98b", "config": {"column": null, "disabled": false, "hide_code": true}, "id": "dNNg", "name": "_"}, {"code": "model_compiled = torch.compile(model)", "code_hash": "1531cfaf0c18385a4736ccecf9706f2c", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "yCnT", "name": "_"}, {"code": "mo.md(r\"\"\"\n## Training the Model\n\nNow that we have prepared training and validation data, and a model, it's time to train our model with our training data, and verify it with its validation data.\n\nThis is called fitting and we need to add two functions to help us with this process.\n\n## Loss and Optimization\n\nThe loss function measures the difference between the model's prediction and the target. The optimizer updates the model's parameters to reduce the loss. In this example we will use  a loss function called [CrossEntropy](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) which is designed to grade if a model predicted the correct category from a group of categories.\n\"\"\")", "code_hash": "c2c38ef326f1f79e4bc0170a614766d5", "config": {"column": null, "disabled": false, "hide_code": true}, "id": "wlCL", "name": "_"}, {"code": "loss_function = nn.CrossEntropyLoss()", "code_hash": "6ca879459dd4b4e59b78c707b74c3a00", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "kqZH", "name": "_"}, {"code": "mo.md(r\"\"\"\nThe optimizer is used to update the model's weights based on the data it sees and the loss function. We will use the [Adam](https://pytorch.org/docs/stable/optim.html) optimizer, which is a popular optimizer in deep learning. It needs to know the models parameters so it can update them.\n\"\"\")", "code_hash": "aa44dd98ac619da88ef5059b2d40399e", "config": {"column": null, "disabled": false, "hide_code": true}, "id": "wAgl", "name": "_"}, {"code": "optimizer = Adam(model_compiled.parameters())", "code_hash": "35f2dc7599a3f9fa33bb208b446e6500", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "rEll", "name": "_"}, {"code": "mo.md(r\"\"\"\n## Accuracy\n\nWhen training a model, it is important to know how well it is performing. We can calculate the accuracy of the model by comparing the model's prediction to the actual target. The simplest way to do this is to compare the model's prediction to the target and calculate the percentage of correct predictions.\n\nWe need to generate our own accuracy functions as these are typically dependent on the problem being solved.\n\nWe need to compare the number of correct classifications compared to the total number of predictions made. Since we're showing data to the model in batches, our accuracy can be calculated along with these batches.\n\nWe typically use N as a postfix to denote the number of samples in a dataset. We can use this to calculate the accuracy of our model.\n\"\"\")", "code_hash": "e998c52ae223b95d055c837dc18e498b", "config": {"column": null, "disabled": false, "hide_code": true}, "id": "dGlV", "name": "_"}, {"code": "train_N = len(train_loader.dataset)\nvalid_N = len(valid_loader.dataset)", "code_hash": "b5c4da8d232f27f05e1c8bf08708973f", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "SdmI", "name": "_"}, {"code": "mo.md(r\"\"\"\nWe can then accumulate the accuracy for each batch and divide by the total number of samples to get the overall accuracy.\n\"\"\")", "code_hash": "06a015f38a7de585a532e9f8b2d40ea4", "config": {"column": null, "disabled": false, "hide_code": true}, "id": "lgWD", "name": "_"}, {"code": "def get_batch_accuracy(output, y, N):\n    pred = output.argmax(dim=1, keepdim=True)\n    correct = pred.eq(y.view_as(pred)).sum().item()\n    return correct / N", "code_hash": "0a394c0ea03f09942311179f9baaa212", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "yOPj", "name": "*get_batch_accuracy"}, {"code": "mo.md(r\"\"\"\n## The Training Loop\n\nWe now generate a function that will train the model for a single epoch. This is similar to the approch we used in the the Linear example but we have added a few more steps to calculate the accuracy of the model.\n\"\"\")", "code_hash": "b2d13789ba97feef993333e96bfd34ee", "config": {"column": null, "disabled": false, "hide_code": true}, "id": "fwwy", "name": "_"}, {"code": "def train():\n    loss = 0\n    accuracy = 0\n    model_compiled.train()\n    for x, y in train_loader:\n        x, y = (x.to(device), y.to(device))\n        output = model_compiled(x)\n        optimizer.zero_grad()\n        batch_loss = loss_function(output, y)\n        batch_loss.backward()\n        optimizer.step()\n        loss = loss + batch_loss.item()\n        accuracy = accuracy + get_batch_accuracy(output, y, train_N)\n    print(\"Train - Loss: {:.4f} Accuracy: {:.4f}\".format(loss, accuracy))", "code_hash": "c3b4685652c102d86ca4fc753ce5170e", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "LJZf", "name": "_"}, {"code": "mo.md(r\"\"\"\n## Validation\n\nOnce the model has done a training step we need to see how close we are to the correct answer. We can do this by running the model on the validation data and calculating the loss and accuracy of the model on the validation data. Agin we will use a function to do this.\n\"\"\")", "code_hash": "374b2e3527b630a610d1430d81ad8f0c", "config": {"column": null, "disabled": false, "hide_code": true}, "id": "urSm", "name": "_"}, {"code": "def validate():\n    loss = 0\n    accuracy = 0\n    model_compiled.eval()\n    with torch.no_grad():\n        for x, y in valid_loader:\n            x, y = (x.to(device), y.to(device))\n            output = model_compiled(x)\n            loss = loss + loss_function(output, y).item()\n            accuracy = accuracy + get_batch_accuracy(output, y, valid_N)\n    print(\"Valid - Loss: {:.4f} Accuracy: {:.4f}\".format(loss, accuracy))", "code_hash": "984a0f8968622293f230f69b00177299", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "jxvo", "name": "_"}, {"code": "mo.md(r\"\"\"\n## The Training loop\n\nWe can now create a training loop that will train the model for a number of epochs. An `epoch` is one complete pass through the entire dataset. Let's train and validate the model for 5 `epochs` to see how it learns.\n\"\"\")", "code_hash": "c47cfe4d10511e01340beaf7544bf795", "config": {"column": null, "disabled": false, "hide_code": true}, "id": "mWxS", "name": "_"}, {"code": "epochs = 10\n\nfor epoch in range(epochs):\n    print(\"Epoch: {}\".format(epoch))\n    train()\n    validate()", "code_hash": "5f412aef9b404b33e8a91228d2d21712", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "CcZR", "name": "_"}, {"code": "mo.md(r\"\"\"\nWe can see that we are quite close (nearly 100%) so we can try and test our model on some existing data.\n\"\"\")", "code_hash": "e1fdb55a07b9149b247fd9f0e923fc8c", "config": {"column": null, "disabled": false, "hide_code": true}, "id": "YWSi", "name": "_"}, {"code": "prediction = model_compiled(test_images_tensor[0].to(device).unsqueeze(0))\nprediction", "code_hash": "5775e9b0fc9e2ba1785ac700a8b2635c", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "zlud", "name": "_"}, {"code": "mo.md(r\"\"\"\nThere should be ten numbers, each corresponding to a different output neuron. Thanks to how the data is structured, the index of each number matches the corresponding handwritten number. The 0th index is a prediction for a handwritten 0, the 1st index is a prediction for a handwritten 1, and so on.\n\nWe can use the `argmax` function to find the index of the highest value.\n\"\"\")", "code_hash": "6eff99a159eafc6ce123656a974d767d", "config": {"column": null, "disabled": false, "hide_code": true}, "id": "tZnO", "name": "_"}, {"code": "print(prediction.argmax(dim=1, keepdim=True))\ndisplay_image(test_images[0], prediction.argmax(dim=1, keepdim=True).item())", "code_hash": "7aa53abbf3b4d4cc4dcb38dba9b0e9f7", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "xvXZ", "name": "_"}, {"code": "mo.md(r\"\"\"\nThis seems to have worked, we should now save the model so we can use it later.\n\n## Saving the Model\n\nWe can save the model using the ```torch.save``` function. We can save the model to a file called `digits.pth` in the current directory. Note we save the original uncompiled model for simplicity as when compiled names get changed. We can always re-compile later if required.\n\"\"\")", "code_hash": "5d9934f33bee78b217c00df7c3976f82", "config": {"column": null, "disabled": false, "hide_code": true}, "id": "CLip", "name": "_"}, {"code": "torch.save(model.state_dict(), \"mnist_model.pth\")\nmodel2 = nn.Sequential(*layers_3)\nmodel2.load_state_dict(torch.load(\"mnist_model.pth\"))\nmodel2.to(device)\nmodel2.eval()\nprediction_1 = model2(test_images_tensor[0].to(device).unsqueeze(0))\ntorch.save(test_images_tensor[0], \"test_image.pth\")\ndisplay_image(test_images[0], prediction_1.argmax(dim=1, keepdim=True).item())", "code_hash": "85026641f6ec44ba810081e64f7c3454", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "YECM", "name": "_"}, {"code": "mo.md(r\"\"\"\nIn the above case we saved only the model state dictionary. We can also save the entire model including the architecture and the state dictionary. This is a little more rigid as it requires the model to be defined in the same way when it is loaded, however we don't also need to re-create the model architecture when we load the model.\n\"\"\")", "code_hash": "347587bec12ff534201d507e9f04c1b0", "config": {"column": null, "disabled": false, "hide_code": true}, "id": "cEAS", "name": "_"}, {"code": "torch.save(model, \"minst_model_full.pth\")\nmodel3 = torch.load(\"minst_model_full.pth\", weights_only=False)\nmodel3.to(device)\nmodel3.eval()\nprediction_2 = model3(test_images_tensor[0].to(device).unsqueeze(0))\ndisplay_image(test_images[0], prediction_2.argmax(dim=1, keepdim=True).item())", "code_hash": "6f0cff18bda1209db102c0a76a3768ca", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "iXej", "name": "_"}, {"code": "mo.md(r\"\"\"\nWe can now use this model to classify new images of digits. We will demonstrate this in a stand alone Qt applications.\n\"\"\")", "code_hash": "682ba05b6ab1e0207a60b08b6f4e1dfc", "config": {"column": null, "disabled": false, "hide_code": true}, "id": "EJmg", "name": "_"}, {"code": "mo.md(r\"\"\"\n## Conclusion\n\nIn this notebook we have trained a neural network to recognize handwritten digits. We have used the MNIST dataset to train the model and have used a simple neural network with a single hidden layer to classify the images. We have trained the model for 5 epochs and have achieved an accuracy of nearly 100%. We have saved the model so we can use it later.\n\nThis is basically the process we will use for all of our machine learning models. We will load the data, create a model, train the model and then save the model. We can then use the model to classify new data.\n\"\"\")", "code_hash": "1e8192779d55628fcc5ede7b7970341f", "config": {"column": null, "disabled": false, "hide_code": true}, "id": "UmEG", "name": "_"}, {"code": "import marimo as mo", "code_hash": "1d0db38904205bec4d6f6f6a1f6cec3e", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "vEBW", "name": "_"}], "metadata": {"marimo_version": "0.17.7"}, "version": "1"},
            "session": {"cells": [{"code_hash": "66f31c9256cd66a3c4b20df893c26e5f", "console": [], "id": "Hbol", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch1 id=\"mnist-digits\"\u003EMNIST Digits\u003C/h1\u003E\n\u003Cspan class=\"paragraph\"\u003EIn this notebook we are going to train a neural network to recognize handwritten digits. This is the  \"Hello World\" of deep learning: training a deep learning model to correctly classify hand-written digits.\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003EIn the previous \u003Ca href=\"TheMNISTDataSet.ipynb\"\u003Enotebook\u003C/a\u003E we downloaded the MNIST dataset, which is a dataset of 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images.  We will re-use this data (downloaded either to your local hard drive or /transfer) to train a neural network to recognize the digits.\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003EWe will start by importing the necessary libraries, including our Utils module.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "d764246e87a6a92079a807799703ffbd", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "mlutils.in_lab()=False\n", "type": "stream"}], "id": "MJUe", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "f209ec4d4d52fc6d52e5bc9028e3c30d", "console": [], "id": "vblA", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch2 id=\"gpu-support\"\u003EGPU Support\u003C/h2\u003E\n\u003Cspan class=\"paragraph\"\u003EIn this notebook we will use the GPU to train our model, we can use the function from our Utils module to check if the GPU is available and set this as the device to use for our data.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "84beed4487e172d5a3c6e183007ba805", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "mps\n", "type": "stream"}], "id": "bkHC", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "1b053b19b189db9368b267b982ff8f2a", "console": [], "id": "lEQa", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch2 id=\"image-classification\"\u003EImage classification\u003C/h2\u003E\n\u003Cspan class=\"paragraph\"\u003EThe approach we are going to take with this example is to load a set of know images and their labels, and train a neural network to learn the relationship between the images and their labels.\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003EWe will use a neural network and a trial and error system to begin to recognize the patterns in the images. The images are small (28x28 pixels) and the neural network will learn to recognize the patterns in the images that are associated with the digits.\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003EWe have a set of 60,000 images to train the network and a separate set of 10,000 images to test the network and on each step of the training process we will check the accuracy of the network on the test set.\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003EIn the previous notebook we created two functions for loading the data and labels so we will use these functions to load the data and labels for the training and test sets.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "e76b3765a5811dd1d45ebcefac073255", "console": [], "id": "PKri", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "793637d94bd27aded8091af070822a5f", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "60000 10000\n5 7\n", "type": "stream"}], "id": "Xref", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "5dce7eb5bb282fedb2d6754b5735bb32", "console": [], "id": "SFPL", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003ETo see the images we can define a simple function to display the images. We will use the matplotlib library to display the images.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "970c1edcfdeeebdc09fec1b62c50dbdd", "console": [{"data": "data:image/png;base64,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", "mimetype": "image/png", "name": "media", "type": "streamMedia"}, {"mimetype": "text/plain", "name": "stdout", "text": "\u003Cclass 'numpy.ndarray'\u003E\n(28, 28)\nuint8\n", "type": "stream"}], "id": "BYtC", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "ba5fa201bb5e36c8c372a6f7d4f832d6", "console": [], "id": "RGSE", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003EIf we look at the data it is stored in a numpy array of 28,28 and a single unsigned char data type. We need to transform this data into the correct type for machine learning. In particular we need to convert the data into a Tensor of type float32, then we need to batch the data into a DataLoader.\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003EWe can use the torchvision library to transform our data as follows.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "6cd5fe55c9d18b15f04b2e1589e59979", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "torch.Size([1, 28, 28])\ntorch.float32\ntensor(0.) tensor(1.)\ncpu\n", "type": "stream"}], "id": "Kclp", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "9299a2c768f59430b1aa6205f3fe561b", "console": [], "id": "emfo", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003EBy default the data for this tensor is processed on the CPU, we can convert it to run on the GPU by using the .to(device) method.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "480b1eaa889d09b0c84dfae4104e39d6", "console": [], "id": "Hstk", "outputs": [{"data": {"text/html": "\u003Cpre style='font-size: 12px'\u003Edevice(type=\u0026#x27;mps\u0026#x27;, index=0)\u003C/pre\u003E"}, "type": "data"}]}, {"code_hash": "91138c30b5768e2d22c9f9a748eb4899", "console": [], "id": "nWHF", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch2 id=\"preparing-the-data-for-training\"\u003EPreparing the Data for Training\u003C/h2\u003E\n\u003Cspan class=\"paragraph\"\u003EEarlier, we created a \u003Ccode\u003Etrans\u003C/code\u003E variable to convert our ndarray to a tensor. \u003Ca href=\"https://pytorch.org/vision/stable/transforms.html\" rel=\"noopener\" target=\"_blank\"\u003ETransforms\u003C/a\u003E are a group of torchvision functions that can be used to transform a dataset.\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003EAt present our train_images and test_images are numpy arrays. We need to convert them to tensors. We can do this using the \u003Ccode\u003Etrans\u003C/code\u003E variable we created earlier.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "6aca7f40bdd762e929f5e74d5e3fbea5", "console": [], "id": "iLit", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "e54109fe61a308c068b9ba208d458d3b", "console": [], "id": "ZHCJ", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch2 id=\"dataloaders\"\u003EDataloaders\u003C/h2\u003E\n\u003Cspan class=\"paragraph\"\u003EWe can use the DataLoader class from the torch.utils.data module to create a DataLoader for our training and test data, you can think of this as batching images into smaller groups for training.\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003EFirst we need to create a custom class to hold our data, we can do this by creating a subclass of the Dataset class from the torch.utils.data module. We need to implement the \u003Cstrong\u003Elen\u003C/strong\u003E and \u003Cstrong\u003Egetitem\u003C/strong\u003E methods to return the length of the dataset and the data and label for a given index.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "94bfd9930f48269b8245c422f80d8fd1", "console": [], "id": "ROlb", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "c3af0c0d266d8ee17dd2f3883c4986a3", "console": [], "id": "qnkX", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003EWe could show our models the entire dataset at once. Not only does this take a lot of computational resources, but \u003Ca href=\"https://arxiv.org/pdf/1804.07612\" rel=\"noopener\" target=\"_blank\"\u003Eresearch shows\u003C/a\u003E using a smaller batch of data is more efficient for model training.\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003EFor example, if our \u003Ccode\u003Ebatch_size\u003C/code\u003E is 32, we will train our model by shuffling the deck and drawing 32 cards. We do not need to shuffle for validation as the model is not learning, but we will still use a \u003Ccode\u003Ebatch_size\u003C/code\u003E to prevent memory errors.\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003EThe batch size is something the model developer decides, and the best value will depend on the problem being solved. Research shows 32 or 64 is sufficient for many machine learning problems and is the default in some machine learning frameworks, so we will use 32 here.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "01d502382d079391ced9941a2cacdf85", "console": [], "id": "TqIu", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "521fdc426713fac3c7cc0eb6e889b491", "console": [], "id": "Vxnm", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch2 id=\"creating-a-model\"\u003ECreating a Model\u003C/h2\u003E\n\u003Cspan class=\"paragraph\"\u003ENeural networks are composed of layers where each layer performs a mathematical operation on the data it receives before passing it to the next layer. To start, we will create a \"Hello World\" level model made from 4 components:\u003C/span\u003E\n\u003Col\u003E\n\u003Cli\u003EA \u003Ca href=\"https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html\" rel=\"noopener\" target=\"_blank\"\u003EFlatten\u003C/a\u003E used to convert n-dimensional data into a vector.\u003C/li\u003E\n\u003Cli\u003EAn input layer, the first layer of neurons\u003C/li\u003E\n\u003Cli\u003EA hidden layer, another layer of neurons \"hidden\" between the input and output\u003C/li\u003E\n\u003Cli\u003EAn output layer, the last set of neurons which returns the final prediction from the model\u003C/li\u003E\n\u003C/ol\u003E\n\u003Cspan class=\"paragraph\"\u003EWe will use a variable called layers to store the layers of our model.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "427c3c186ab9e25f36752a2be24309eb", "console": [], "id": "DnEU", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch2 id=\"flatten-the-image\"\u003EFlatten the image\u003C/h2\u003E\n\u003Cspan class=\"paragraph\"\u003EThe first thing we need to do is convert the image from a 28x28  array into a flat 1d tensor. We saw the images had 3 dimensions: \u003Ccode\u003EC x H x W\u003C/code\u003E. To flatten an image means to combine all of these images into 1 dimension. Let's say we have a tensor like the one below.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "1a326d870c3dd07a1145e88fbd87d929", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "tensor([[1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]])\ntensor([[1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]])\n", "type": "stream"}], "id": "ulZA", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "99e75a78359e68b5f496ec78e2af2d3e", "console": [], "id": "ecfG", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003EYou will notice nothing happened, this is because neural networks expect to receive a batch of data. Currently, the Flatten layer sees three vectors as opposed to one 2d matrix. To fix this, we can \"batch\" our data by adding an extra pair of brackets. Since \u003Ccode\u003Etest_matrix\u003C/code\u003E is now a tensor, we can do that with the shorthand below. \u003Ccode\u003ENone\u003C/code\u003E adds a new dimension where \u003Ccode\u003E:\u003C/code\u003E selects all the data in a tensor.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "6a2b7a5792f1a9ef29d1a585765ca0f4", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "tensor([[[1, 2, 3],\n         [4, 5, 6],\n         [7, 8, 9]]])\ntensor([[1, 2, 3, 4, 5, 6, 7, 8, 9]])\n", "type": "stream"}], "id": "Pvdt", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "d969f5841c1b0d216cd8ab3e70cce476", "console": [], "id": "ZBYS", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch2 id=\"the-input-layer\"\u003EThe Input Layer\u003C/h2\u003E\n\u003Cspan class=\"paragraph\"\u003EThe input layer is the first layer of neurons in the neural network. It is responsible for receiving the input data and passing it to the next layer.\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003EThis layer will be \u003Cem\u003Edensely connected\u003C/em\u003E, meaning that each neuron in it, and its weights, will affect every neuron in the next layer.\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003EIn order to create these weights, Pytorch needs to know the size of our inputs and how many neurons we want to create.\nSince we've flattened our images, the size of our inputs is the number of channels, number of pixels vertically, and number of pixels horizontally multiplied together.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "8a6b26fa192c199e71842549713fa50f", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "784\n", "type": "stream"}], "id": "aLJB", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "984af51c3bd17946b5c3536e59c29cc5", "console": [], "id": "nHfw", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003EChoosing the correct number of neurons is what puts the \"science\" in \"data science\" as it is a matter of capturing the statistical complexity of the dataset. For now, we will use \u003Ccode\u003E512\u003C/code\u003E neurons. Try playing around with this value later to see how it affects training and to start developing a sense for what this number means.\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003EWe will learn more about activation functions later, but for now, we will use the \u003Ca href=\"https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html\" rel=\"noopener\" target=\"_blank\"\u003Erelu\u003C/a\u003E activation function, which in short, will help our network to learn how to make more sophisticated guesses about data than if it were required to make guesses based on some strictly linear function.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "51a69ee8bf0787cc4b7a139750b9d43a", "console": [], "id": "xXTn", "outputs": [{"data": {"application/json": "[\"text/plain:Flatten(start_dim=1, end_dim=-1)\", \"text/plain:Linear(in_features=784, out_features=512, bias=True)\", \"text/plain:ReLU()\"]"}, "type": "data"}]}, {"code_hash": "1393beeec9f4088acc139841199371c4", "console": [], "id": "AjVT", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch2 id=\"the-hidden-layer\"\u003EThe hidden layer\u003C/h2\u003E\n\u003Cspan class=\"paragraph\"\u003EA hidden layer is a layer of neurons between the input and output layers. It is called \"hidden\" because it is not directly exposed to the input data and the output predictions. We will cover why we combine multiple layers in another lecture, but for now, we will add a hidden layer to our model.\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003EAs with the previous layers, the shape of the data is important. \u003Ca href=\"https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\" rel=\"noopener\" target=\"_blank\"\u003Enn.Linear\u003C/a\u003E needs to know the shape of the data being passed to it. Each neuron in the previous layer will compute one number, so the number of inputs into the hidden layer is the same as the number of neurons in the previous later.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "ddabf47c974f5d80bb33c57e0257e71d", "console": [], "id": "pHFh", "outputs": [{"data": {"application/json": "[\"text/plain:Flatten(start_dim=1, end_dim=-1)\", \"text/plain:Linear(in_features=784, out_features=512, bias=True)\", \"text/plain:ReLU()\", \"text/plain:Linear(in_features=512, out_features=512, bias=True)\", \"text/plain:ReLU()\"]"}, "type": "data"}]}, {"code_hash": "835c457b3848f1e0325f140c7c8d6bdb", "console": [], "id": "NCOB", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch2 id=\"the-output-layer\"\u003EThe Output Layer\u003C/h2\u003E\n\u003Cspan class=\"paragraph\"\u003EThe output layer is the final layer of neurons in the neural network. It is responsible for producing the output of the model. This will be a tensor of length 10, where each element represents the probability of the input image being a particular digit.\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003EWe will not assign the \u003Ccode\u003Erelu\u003C/code\u003E function to the output layer. Instead, we will apply a \u003Ccode\u003Eloss function\u003C/code\u003E covered in the next section.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "5e0ba0758cb87a84acd07bbcac7b4a3a", "console": [], "id": "aqbW", "outputs": [{"data": {"application/json": "[\"text/plain:Flatten(start_dim=1, end_dim=-1)\", \"text/plain:Linear(in_features=784, out_features=512, bias=True)\", \"text/plain:ReLU()\", \"text/plain:Linear(in_features=512, out_features=512, bias=True)\", \"text/plain:ReLU()\", \"text/plain:Linear(in_features=512, out_features=10, bias=True)\"]"}, "type": "data"}]}, {"code_hash": "e16ad0accc14568fb98956b12895ec23", "console": [], "id": "TRpd", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch2 id=\"compiling-the-model\"\u003ECompiling the Model\u003C/h2\u003E\n\u003Cspan class=\"paragraph\"\u003EA \u003Ca href=\"https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html\" rel=\"noopener\" target=\"_blank\"\u003ESequential\u003C/a\u003E model expects a sequence of arguments, not a list, so we can use the \u003Ca href=\"https://docs.python.org/3/reference/expressions.html#expression-lists\" rel=\"noopener\" target=\"_blank\"\u003E* operator\u003C/a\u003E to unpack our list of layers into a sequence. We can print the model to verify these layers loaded correctly. We can also send our model to the GPU using the to(device) method. We can verify where the model is by using the .device attribute.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "37b9c250c9b9139e2b94d83d66b45a6f", "console": [], "id": "TXez", "outputs": [{"data": {"text/html": "\u003Cpre style='font-size: 12px'\u003Edevice(type=\u0026#x27;mps\u0026#x27;, index=0)\u003C/pre\u003E"}, "type": "data"}]}, {"code_hash": "8f11f663c9d49dbd7b168c892e75e98b", "console": [], "id": "dNNg", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003EPyTorch 2.0 introduced the ability to \u003Ca href=\"[here](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html).\"\u003Ecompile\u003C/a\u003E the model using the \u003Ccode\u003Ecompile\u003C/code\u003E method which can give faster performance.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "1531cfaf0c18385a4736ccecf9706f2c", "console": [], "id": "yCnT", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "c2c38ef326f1f79e4bc0170a614766d5", "console": [], "id": "wlCL", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch2 id=\"training-the-model\"\u003ETraining the Model\u003C/h2\u003E\n\u003Cspan class=\"paragraph\"\u003ENow that we have prepared training and validation data, and a model, it's time to train our model with our training data, and verify it with its validation data.\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003EThis is called fitting and we need to add two functions to help us with this process.\u003C/span\u003E\n\u003Ch2 id=\"loss-and-optimization\"\u003ELoss and Optimization\u003C/h2\u003E\n\u003Cspan class=\"paragraph\"\u003EThe loss function measures the difference between the model's prediction and the target. The optimizer updates the model's parameters to reduce the loss. In this example we will use  a loss function called \u003Ca href=\"https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\" rel=\"noopener\" target=\"_blank\"\u003ECrossEntropy\u003C/a\u003E which is designed to grade if a model predicted the correct category from a group of categories.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "6ca879459dd4b4e59b78c707b74c3a00", "console": [], "id": "kqZH", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "aa44dd98ac619da88ef5059b2d40399e", "console": [], "id": "wAgl", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003EThe optimizer is used to update the model's weights based on the data it sees and the loss function. We will use the \u003Ca href=\"https://pytorch.org/docs/stable/optim.html\" rel=\"noopener\" target=\"_blank\"\u003EAdam\u003C/a\u003E optimizer, which is a popular optimizer in deep learning. It needs to know the models parameters so it can update them.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "35f2dc7599a3f9fa33bb208b446e6500", "console": [], "id": "rEll", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "e998c52ae223b95d055c837dc18e498b", "console": [], "id": "dGlV", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch2 id=\"accuracy\"\u003EAccuracy\u003C/h2\u003E\n\u003Cspan class=\"paragraph\"\u003EWhen training a model, it is important to know how well it is performing. We can calculate the accuracy of the model by comparing the model's prediction to the actual target. The simplest way to do this is to compare the model's prediction to the target and calculate the percentage of correct predictions.\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003EWe need to generate our own accuracy functions as these are typically dependent on the problem being solved.\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003EWe need to compare the number of correct classifications compared to the total number of predictions made. Since we're showing data to the model in batches, our accuracy can be calculated along with these batches.\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003EWe typically use N as a postfix to denote the number of samples in a dataset. We can use this to calculate the accuracy of our model.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "b5c4da8d232f27f05e1c8bf08708973f", "console": [], "id": "SdmI", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "06a015f38a7de585a532e9f8b2d40ea4", "console": [], "id": "lgWD", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003EWe can then accumulate the accuracy for each batch and divide by the total number of samples to get the overall accuracy.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "0a394c0ea03f09942311179f9baaa212", "console": [], "id": "yOPj", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "b2d13789ba97feef993333e96bfd34ee", "console": [], "id": "fwwy", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch2 id=\"the-training-loop\"\u003EThe Training Loop\u003C/h2\u003E\n\u003Cspan class=\"paragraph\"\u003EWe now generate a function that will train the model for a single epoch. This is similar to the approch we used in the the Linear example but we have added a few more steps to calculate the accuracy of the model.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "c3b4685652c102d86ca4fc753ce5170e", "console": [], "id": "LJZf", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "374b2e3527b630a610d1430d81ad8f0c", "console": [], "id": "urSm", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch2 id=\"validation\"\u003EValidation\u003C/h2\u003E\n\u003Cspan class=\"paragraph\"\u003EOnce the model has done a training step we need to see how close we are to the correct answer. We can do this by running the model on the validation data and calculating the loss and accuracy of the model on the validation data. Agin we will use a function to do this.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "984a0f8968622293f230f69b00177299", "console": [], "id": "jxvo", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "c47cfe4d10511e01340beaf7544bf795", "console": [], "id": "mWxS", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch2 id=\"the-training-loop\"\u003EThe Training loop\u003C/h2\u003E\n\u003Cspan class=\"paragraph\"\u003EWe can now create a training loop that will train the model for a number of epochs. An \u003Ccode\u003Eepoch\u003C/code\u003E is one complete pass through the entire dataset. Let's train and validate the model for 5 \u003Ccode\u003Eepochs\u003C/code\u003E to see how it learns.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "5f412aef9b404b33e8a91228d2d21712", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "Epoch: 0\n", "type": "stream"}, {"mimetype": "text/plain", "name": "stdout", "text": "Train - Loss: 667.8434 Accuracy: 0.9235\n", "type": "stream"}, {"mimetype": "text/plain", "name": "stdout", "text": "Valid - Loss: 58.5437 Accuracy: 0.9511\nEpoch: 1\n", "type": "stream"}, {"mimetype": "text/plain", "name": "stdout", "text": "Train - Loss: 324.6180 Accuracy: 0.9530\n", "type": "stream"}, {"mimetype": "text/plain", "name": "stdout", "text": "Valid - Loss: 45.5673 Accuracy: 0.9617\nEpoch: 2\n", "type": "stream"}, {"mimetype": "text/plain", "name": "stdout", "text": "Train - Loss: 298.6441 Accuracy: 0.9587\n", "type": "stream"}, {"mimetype": "text/plain", "name": "stdout", "text": "Valid - Loss: 47.1065 Accuracy: 0.9618\nEpoch: 3\n", "type": "stream"}, {"mimetype": "text/plain", "name": "stdout", "text": "Train - Loss: 253.2819 Accuracy: 0.9649\n", "type": "stream"}, {"mimetype": "text/plain", "name": "stdout", "text": "Valid - Loss: 44.5172 Accuracy: 0.9689\nEpoch: 4\n", "type": "stream"}, {"mimetype": "text/plain", "name": "stdout", "text": "Train - Loss: 223.2464 Accuracy: 0.9687\n", "type": "stream"}, {"mimetype": "text/plain", "name": "stdout", "text": "Valid - Loss: 45.9954 Accuracy: 0.9648\nEpoch: 5\n", "type": "stream"}, {"mimetype": "text/plain", "name": "stdout", "text": "Train - Loss: 193.8764 Accuracy: 0.9731\n", "type": "stream"}, {"mimetype": "text/plain", "name": "stdout", "text": "Valid - Loss: 49.3357 Accuracy: 0.9708\nEpoch: 6\n", "type": "stream"}, {"mimetype": "text/plain", "name": "stdout", "text": "Train - Loss: 185.5772 Accuracy: 0.9743\n", "type": "stream"}, {"mimetype": "text/plain", "name": "stdout", "text": "Valid - Loss: 49.8743 Accuracy: 0.9676\nEpoch: 7\n", "type": "stream"}, {"mimetype": "text/plain", "name": "stdout", "text": "Train - Loss: 174.1708 Accuracy: 0.9770\n", "type": "stream"}, {"mimetype": "text/plain", "name": "stdout", "text": "Valid - Loss: 43.1488 Accuracy: 0.9717\nEpoch: 8\n", "type": "stream"}, {"mimetype": "text/plain", "name": "stdout", "text": "Train - Loss: 157.0738 Accuracy: 0.9785\n", "type": "stream"}, {"mimetype": "text/plain", "name": "stdout", "text": "Valid - Loss: 68.2520 Accuracy: 0.9640\nEpoch: 9\n", "type": "stream"}, {"mimetype": "text/plain", "name": "stdout", "text": "Train - Loss: 151.3510 Accuracy: 0.9799\n", "type": "stream"}, {"mimetype": "text/plain", "name": "stdout", "text": "Valid - Loss: 59.3690 Accuracy: 0.9668\n", "type": "stream"}], "id": "CcZR", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "e1fdb55a07b9149b247fd9f0e923fc8c", "console": [], "id": "YWSi", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003EWe can see that we are quite close (nearly 100%) so we can try and test our model on some existing data.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "5775e9b0fc9e2ba1785ac700a8b2635c", "console": [], "id": "zlud", "outputs": [{"data": {"text/html": "\u003Cpre style='font-size: 12px'\u003Etensor([[-33.2033, -32.3519,   1.4694, -20.8904,  -8.9273, -35.2868, -65.4364,\n          26.6204, -54.7452,   1.2547]], device=\u0026#x27;mps:0\u0026#x27;,\n       grad_fn=\u0026lt;CompiledFunctionBackward\u0026gt;)\u003C/pre\u003E"}, "type": "data"}]}, {"code_hash": "6eff99a159eafc6ce123656a974d767d", "console": [], "id": "tZnO", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003EThere should be ten numbers, each corresponding to a different output neuron. Thanks to how the data is structured, the index of each number matches the corresponding handwritten number. The 0th index is a prediction for a handwritten 0, the 1st index is a prediction for a handwritten 1, and so on.\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003EWe can use the \u003Ccode\u003Eargmax\u003C/code\u003E function to find the index of the highest value.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "7aa53abbf3b4d4cc4dcb38dba9b0e9f7", "console": [{"data": "data:image/png;base64,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", "mimetype": "image/png", "name": "media", "type": "streamMedia"}, {"mimetype": "text/plain", "name": "stdout", "text": "tensor([[7]], device='mps:0')\n", "type": "stream"}], "id": "xvXZ", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "5d9934f33bee78b217c00df7c3976f82", "console": [], "id": "CLip", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003EThis seems to have worked, we should now save the model so we can use it later.\u003C/span\u003E\n\u003Ch2 id=\"saving-the-model\"\u003ESaving the Model\u003C/h2\u003E\n\u003Cspan class=\"paragraph\"\u003EWe can save the model using the \u003Ccode\u003Etorch.save\u003C/code\u003E function. We can save the model to a file called \u003Ccode\u003Edigits.pth\u003C/code\u003E in the current directory. Note we save the original uncompiled model for simplicity as when compiled names get changed. We can always re-compile later if required.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "85026641f6ec44ba810081e64f7c3454", "console": [{"data": "data:image/png;base64,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", "mimetype": "image/png", "name": "media", "type": "streamMedia"}], "id": "YECM", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "347587bec12ff534201d507e9f04c1b0", "console": [], "id": "cEAS", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003EIn the above case we saved only the model state dictionary. We can also save the entire model including the architecture and the state dictionary. This is a little more rigid as it requires the model to be defined in the same way when it is loaded, however we don't also need to re-create the model architecture when we load the model.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "6f0cff18bda1209db102c0a76a3768ca", "console": [{"data": "data:image/png;base64,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", "mimetype": "image/png", "name": "media", "type": "streamMedia"}], "id": "iXej", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "682ba05b6ab1e0207a60b08b6f4e1dfc", "console": [], "id": "EJmg", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003EWe can now use this model to classify new images of digits. We will demonstrate this in a stand alone Qt applications.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "1e8192779d55628fcc5ede7b7970341f", "console": [], "id": "UmEG", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch2 id=\"conclusion\"\u003EConclusion\u003C/h2\u003E\n\u003Cspan class=\"paragraph\"\u003EIn this notebook we have trained a neural network to recognize handwritten digits. We have used the MNIST dataset to train the model and have used a simple neural network with a single hidden layer to classify the images. We have trained the model for 5 epochs and have achieved an accuracy of nearly 100%. We have saved the model so we can use it later.\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003EThis is basically the process we will use for all of our machine learning models. We will load the data, create a model, train the model and then save the model. We can then use the model to classify new data.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "1d0db38904205bec4d6f6f6a1f6cec3e", "console": [], "id": "vEBW", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}], "metadata": {"marimo_version": "0.17.7"}, "version": "1"},
            "runtimeConfig": null,
        };
    </script>
  
<marimo-code hidden="">
    import%20marimo%0A%0A__generated_with%20%3D%20%220.17.7%22%0Aapp%20%3D%20marimo.App(width%3D%22full%22%2C%20app_title%3D%22MNIST%20Digits%22)%0A%0A%0A%40app.cell(hide_code%3DTrue)%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%20MNIST%20Digits%0A%0A%20%20%20%20In%20this%20notebook%20we%20are%20going%20to%20train%20a%20neural%20network%20to%20recognize%20handwritten%20digits.%20This%20is%20the%20%20%22Hello%20World%22%20of%20deep%20learning%3A%20training%20a%20deep%20learning%20model%20to%20correctly%20classify%20hand-written%20digits.%0A%0A%20%20%20%20In%20the%20previous%20%5Bnotebook%5D(TheMNISTDataSet.ipynb)%20we%20downloaded%20the%20MNIST%20dataset%2C%20which%20is%20a%20dataset%20of%2060%2C000%2028x28%20grayscale%20images%20of%20the%2010%20digits%2C%20along%20with%20a%20test%20set%20of%2010%2C000%20images.%20%20We%20will%20re-use%20this%20data%20(downloaded%20either%20to%20your%20local%20hard%20drive%20or%20%2Ftransfer)%20to%20train%20a%20neural%20network%20to%20recognize%20the%20digits.%0A%0A%20%20%20%20We%20will%20start%20by%20importing%20the%20necessary%20libraries%2C%20including%20our%20Utils%20module.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20import%20mlutils%0A%20%20%20%20import%20matplotlib.pyplot%20as%20plt%0A%20%20%20%20import%20numpy%20as%20np%0A%20%20%20%20import%20struct%0A%20%20%20%20import%20sys%0A%20%20%20%20import%20torch%0A%20%20%20%20import%20torch.nn%20as%20nn%0A%20%20%20%20from%20torch.optim%20import%20Adam%0A%20%20%20%20from%20torch.utils.data%20import%20Dataset%2C%20DataLoader%0A%0A%20%20%20%20%23%20Visualization%20tools%0A%20%20%20%20import%20torchvision.transforms.v2%20as%20transforms%0A%0A%20%20%20%20print(f%22%7Bmlutils.in_lab()%3D%7D%22)%0A%20%20%20%20return%20(%0A%20%20%20%20%20%20%20%20Adam%2C%0A%20%20%20%20%20%20%20%20DataLoader%2C%0A%20%20%20%20%20%20%20%20Dataset%2C%0A%20%20%20%20%20%20%20%20mlutils%2C%0A%20%20%20%20%20%20%20%20nn%2C%0A%20%20%20%20%20%20%20%20np%2C%0A%20%20%20%20%20%20%20%20plt%2C%0A%20%20%20%20%20%20%20%20struct%2C%0A%20%20%20%20%20%20%20%20torch%2C%0A%20%20%20%20%20%20%20%20transforms%2C%0A%20%20%20%20)%0A%0A%0A%40app.cell(hide_code%3DTrue)%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%20GPU%20Support%0A%0A%20%20%20%20In%20this%20notebook%20we%20will%20use%20the%20GPU%20to%20train%20our%20model%2C%20we%20can%20use%20the%20function%20from%20our%20Utils%20module%20to%20check%20if%20the%20GPU%20is%20available%20and%20set%20this%20as%20the%20device%20to%20use%20for%20our%20data.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(mlutils)%3A%0A%20%20%20%20device%20%3D%20mlutils.get_device()%0A%20%20%20%20print(device)%0A%20%20%20%20return%20(device%2C)%0A%0A%0A%40app.cell(hide_code%3DTrue)%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%20Image%20classification%0A%0A%20%20%20%20The%20approach%20we%20are%20going%20to%20take%20with%20this%20example%20is%20to%20load%20a%20set%20of%20know%20images%20and%20their%20labels%2C%20and%20train%20a%20neural%20network%20to%20learn%20the%20relationship%20between%20the%20images%20and%20their%20labels.%0A%0A%20%20%20%20We%20will%20use%20a%20neural%20network%20and%20a%20trial%20and%20error%20system%20to%20begin%20to%20recognize%20the%20patterns%20in%20the%20images.%20The%20images%20are%20small%20(28x28%20pixels)%20and%20the%20neural%20network%20will%20learn%20to%20recognize%20the%20patterns%20in%20the%20images%20that%20are%20associated%20with%20the%20digits.%0A%0A%20%20%20%20We%20have%20a%20set%20of%2060%2C000%20images%20to%20train%20the%20network%20and%20a%20separate%20set%20of%2010%2C000%20images%20to%20test%20the%20network%20and%20on%20each%20step%20of%20the%20training%20process%20we%20will%20check%20the%20accuracy%20of%20the%20network%20on%20the%20test%20set.%0A%0A%20%20%20%20In%20the%20previous%20notebook%20we%20created%20two%20functions%20for%20loading%20the%20data%20and%20labels%20so%20we%20will%20use%20these%20functions%20to%20load%20the%20data%20and%20labels%20for%20the%20training%20and%20test%20sets.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(np%2C%20struct)%3A%0A%20%20%20%20def%20load_mnist_labels(filename%3A%20str)%20-%3E%20np.ndarray%3A%0A%20%20%20%20%20%20%20%20with%20open(filename%2C%20%22rb%22)%20as%20f%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20magic%2C%20num%20%3D%20struct.unpack(%22%3EII%22%2C%20f.read(8))%0A%20%20%20%20%20%20%20%20%20%20%20%20labels%20%3D%20np.fromfile(f%2C%20dtype%3Dnp.uint8)%0A%20%20%20%20%20%20%20%20%20%20%20%20if%20len(labels)%20!%3D%20num%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20raise%20ValueError(f%22Expected%20%7Bnum%7D%20labels%2C%20but%20got%20%7Blen(labels)%7D%22)%0A%20%20%20%20%20%20%20%20return%20labels%0A%0A%0A%20%20%20%20def%20load_mnist_images(filename%3A%20str)%20-%3E%20np.ndarray%3A%0A%20%20%20%20%20%20%20%20with%20open(filename%2C%20%22rb%22)%20as%20f%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20magic%2C%20num%2C%20rows%2C%20cols%20%3D%20struct.unpack(%22%3EIIII%22%2C%20f.read(16))%0A%20%20%20%20%20%20%20%20%20%20%20%20images%20%3D%20np.fromfile(f%2C%20dtype%3Dnp.uint8).reshape(num%2C%20rows%2C%20cols)%0A%20%20%20%20%20%20%20%20%20%20%20%20if%20len(images)%20!%3D%20num%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20raise%20ValueError(f%22Expected%20%7Bnum%7D%20images%2C%20but%20got%20%7Blen(images)%7D%22)%0A%20%20%20%20%20%20%20%20return%20images%0A%20%20%20%20return%20load_mnist_images%2C%20load_mnist_labels%0A%0A%0A%40app.cell%0Adef%20_(load_mnist_images%2C%20load_mnist_labels%2C%20mlutils)%3A%0A%20%20%20%20DATASET_LOCATION%20%3D%20%22%22%0A%20%20%20%20if%20mlutils.in_lab()%3A%0A%20%20%20%20%20%20%20%20DATASET_LOCATION%20%3D%20%22%2Ftransfer%2FMNIST_Dataset%2F%22%0A%20%20%20%20else%3A%0A%20%20%20%20%20%20%20%20DATASET_LOCATION%20%3D%20%22.%2FMNIST_Dataset%2F%22%0A%0A%20%20%20%20train_labels%20%3D%20load_mnist_labels(DATASET_LOCATION%20%2B%20%22train-labels-idx1-ubyte%22)%0A%20%20%20%20test_labels%20%3D%20load_mnist_labels(DATASET_LOCATION%20%2B%20%22t10k-labels-idx1-ubyte%22)%0A%0A%20%20%20%20print(len(train_labels)%2C%20len(test_labels))%0A%20%20%20%20print(train_labels%5B0%5D%2C%20test_labels%5B0%5D)%0A%0A%20%20%20%20%23%20We%20can%20now%20load%20the%20images%20from%20both%20the%20datasets.%0A%20%20%20%20train_images%20%3D%20load_mnist_images(DATASET_LOCATION%20%2B%20%22train-images-idx3-ubyte%22)%0A%20%20%20%20test_images%20%3D%20load_mnist_images(DATASET_LOCATION%20%2B%20%22t10k-images-idx3-ubyte%22)%0A%20%20%20%20return%20test_images%2C%20test_labels%2C%20train_images%2C%20train_labels%0A%0A%0A%40app.cell(hide_code%3DTrue)%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20To%20see%20the%20images%20we%20can%20define%20a%20simple%20function%20to%20display%20the%20images.%20We%20will%20use%20the%20matplotlib%20library%20to%20display%20the%20images.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(np%2C%20plt%2C%20train_images%2C%20train_labels)%3A%0A%20%20%20%20def%20display_image(image%3A%20np.array%2C%20label%3A%20str)%20-%3E%20None%3A%0A%20%20%20%20%20%20%20%20plt.figure(figsize%3D(1%2C%201))%0A%20%20%20%20%20%20%20%20plt.title(f%22Label%20%3A%20%7Blabel%7D%22)%0A%20%20%20%20%20%20%20%20plt.imshow(image%2C%20cmap%3D%22gray%22)%0A%20%20%20%20%20%20%20%20plt.axis(%22off%22)%0A%20%20%20%20%20%20%20%20plt.show()%0A%0A%0A%20%20%20%20%23%20We%20can%20now%20display%20the%20first%20image%20from%20the%20training%20dataset.%0A%0A%20%20%20%20display_image(train_images%5B0%5D%2C%20train_labels%5B0%5D)%0A%20%20%20%20print(type(train_images%5B0%5D))%0A%20%20%20%20print(train_images%5B0%5D.shape)%0A%20%20%20%20print(train_images%5B0%5D.dtype)%0A%20%20%20%20return%20(display_image%2C)%0A%0A%0A%40app.cell(hide_code%3DTrue)%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20If%20we%20look%20at%20the%20data%20it%20is%20stored%20in%20a%20numpy%20array%20of%2028%2C28%20and%20a%20single%20unsigned%20char%20data%20type.%20We%20need%20to%20transform%20this%20data%20into%20the%20correct%20type%20for%20machine%20learning.%20In%20particular%20we%20need%20to%20convert%20the%20data%20into%20a%20Tensor%20of%20type%20float32%2C%20then%20we%20need%20to%20batch%20the%20data%20into%20a%20DataLoader.%0A%0A%20%20%20%20We%20can%20use%20the%20torchvision%20library%20to%20transform%20our%20data%20as%20follows.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(torch%2C%20train_images%2C%20transforms)%3A%0A%20%20%20%20trans%20%3D%20transforms.Compose(%0A%20%20%20%20%20%20%20%20%5Btransforms.ToImage()%2C%20transforms.ToDtype(torch.float32%2C%20scale%3DTrue)%5D%0A%20%20%20%20)%0A%20%20%20%20tensor%20%3D%20trans(train_images%5B0%5D)%0A%20%20%20%20print(tensor.shape)%0A%20%20%20%20print(tensor.dtype)%0A%20%20%20%20print(tensor.min()%2C%20tensor.max())%0A%20%20%20%20print(tensor.device)%0A%20%20%20%20return%20(tensor%2C)%0A%0A%0A%40app.cell(hide_code%3DTrue)%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20By%20default%20the%20data%20for%20this%20tensor%20is%20processed%20on%20the%20CPU%2C%20we%20can%20convert%20it%20to%20run%20on%20the%20GPU%20by%20using%20the%20.to(device)%20method.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(device%2C%20tensor)%3A%0A%20%20%20%20tensor.to(device).device%0A%20%20%20%20return%0A%0A%0A%40app.cell(hide_code%3DTrue)%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%20Preparing%20the%20Data%20for%20Training%0A%0A%20%20%20%20Earlier%2C%20we%20created%20a%20%60trans%60%20variable%20to%20convert%20our%20ndarray%20to%20a%20tensor.%20%5BTransforms%5D(https%3A%2F%2Fpytorch.org%2Fvision%2Fstable%2Ftransforms.html)%20are%20a%20group%20of%20torchvision%20functions%20that%20can%20be%20used%20to%20transform%20a%20dataset.%0A%0A%20%20%20%20At%20present%20our%20train_images%20and%20test_images%20are%20numpy%20arrays.%20We%20need%20to%20convert%20them%20to%20tensors.%20We%20can%20do%20this%20using%20the%20%60trans%60%20variable%20we%20created%20earlier.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(device%2C%20test_images%2C%20test_labels%2C%20torch%2C%20train_images%2C%20train_labels)%3A%0A%20%20%20%20train_images_tensor%20%3D%20torch.tensor(train_images%2C%20dtype%3Dtorch.float32).to(%0A%20%20%20%20%20%20%20%20device%0A%20%20%20%20)%0A%20%20%20%20train_labels_tensor%20%3D%20torch.tensor(train_labels%2C%20dtype%3Dtorch.uint8).to(device)%0A%0A%20%20%20%20test_images_tensor%20%3D%20torch.tensor(test_images%2C%20dtype%3Dtorch.float32).to(device)%0A%20%20%20%20test_labels_tensor%20%3D%20torch.tensor(test_labels%2C%20dtype%3Dtorch.uint8).to(device)%0A%20%20%20%20return%20(%0A%20%20%20%20%20%20%20%20test_images_tensor%2C%0A%20%20%20%20%20%20%20%20test_labels_tensor%2C%0A%20%20%20%20%20%20%20%20train_images_tensor%2C%0A%20%20%20%20%20%20%20%20train_labels_tensor%2C%0A%20%20%20%20)%0A%0A%0A%40app.cell(hide_code%3DTrue)%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%20Dataloaders%0A%0A%20%20%20%20We%20can%20use%20the%20DataLoader%20class%20from%20the%20torch.utils.data%20module%20to%20create%20a%20DataLoader%20for%20our%20training%20and%20test%20data%2C%20you%20can%20think%20of%20this%20as%20batching%20images%20into%20smaller%20groups%20for%20training.%0A%0A%20%20%20%20First%20we%20need%20to%20create%20a%20custom%20class%20to%20hold%20our%20data%2C%20we%20can%20do%20this%20by%20creating%20a%20subclass%20of%20the%20Dataset%20class%20from%20the%20torch.utils.data%20module.%20We%20need%20to%20implement%20the%20__len__%20and%20__getitem__%20methods%20to%20return%20the%20length%20of%20the%20dataset%20and%20the%20data%20and%20label%20for%20a%20given%20index.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(Dataset)%3A%0A%20%20%20%20%23%20Custom%20dataset%20class%0A%20%20%20%20class%20DigitsDataset(Dataset)%3A%0A%20%20%20%20%20%20%20%20def%20__init__(self%2C%20images_tensor%2C%20labels_tensor)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20self.images_tensor%20%3D%20images_tensor%0A%20%20%20%20%20%20%20%20%20%20%20%20self.labels_tensor%20%3D%20labels_tensor%0A%0A%20%20%20%20%20%20%20%20def%20__len__(self)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20return%20len(self.labels_tensor)%0A%0A%20%20%20%20%20%20%20%20def%20__getitem__(self%2C%20idx)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20image%20%3D%20self.images_tensor%5Bidx%5D%0A%20%20%20%20%20%20%20%20%20%20%20%20label%20%3D%20self.labels_tensor%5Bidx%5D%0A%20%20%20%20%20%20%20%20%20%20%20%20return%20image%2C%20label%0A%20%20%20%20return%20(DigitsDataset%2C)%0A%0A%0A%40app.cell(hide_code%3DTrue)%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20We%20could%20show%20our%20models%20the%20entire%20dataset%20at%20once.%20Not%20only%20does%20this%20take%20a%20lot%20of%20computational%20resources%2C%20but%20%5Bresearch%20shows%5D(https%3A%2F%2Farxiv.org%2Fpdf%2F1804.07612)%20using%20a%20smaller%20batch%20of%20data%20is%20more%20efficient%20for%20model%20training.%0A%0A%20%20%20%20For%20example%2C%20if%20our%20%60batch_size%60%20is%2032%2C%20we%20will%20train%20our%20model%20by%20shuffling%20the%20deck%20and%20drawing%2032%20cards.%20We%20do%20not%20need%20to%20shuffle%20for%20validation%20as%20the%20model%20is%20not%20learning%2C%20but%20we%20will%20still%20use%20a%20%60batch_size%60%20to%20prevent%20memory%20errors.%0A%0A%20%20%20%20The%20batch%20size%20is%20something%20the%20model%20developer%20decides%2C%20and%20the%20best%20value%20will%20depend%20on%20the%20problem%20being%20solved.%20Research%20shows%2032%20or%2064%20is%20sufficient%20for%20many%20machine%20learning%20problems%20and%20is%20the%20default%20in%20some%20machine%20learning%20frameworks%2C%20so%20we%20will%20use%2032%20here.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(%0A%20%20%20%20DataLoader%2C%0A%20%20%20%20DigitsDataset%2C%0A%20%20%20%20test_images_tensor%2C%0A%20%20%20%20test_labels_tensor%2C%0A%20%20%20%20train_images_tensor%2C%0A%20%20%20%20train_labels_tensor%2C%0A)%3A%0A%20%20%20%20batch_size%20%3D%2032%0A%0A%20%20%20%20train_data%20%3D%20DigitsDataset(train_images_tensor%2C%20train_labels_tensor)%0A%20%20%20%20valid_data%20%3D%20DigitsDataset(test_images_tensor%2C%20test_labels_tensor)%0A%0A%20%20%20%20train_loader%20%3D%20DataLoader(train_data%2C%20batch_size%3Dbatch_size%2C%20shuffle%3DTrue)%0A%20%20%20%20valid_loader%20%3D%20DataLoader(valid_data%2C%20batch_size%3Dbatch_size)%0A%20%20%20%20return%20train_loader%2C%20valid_loader%0A%0A%0A%40app.cell(hide_code%3DTrue)%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%20Creating%20a%20Model%0A%0A%20%20%20%20Neural%20networks%20are%20composed%20of%20layers%20where%20each%20layer%20performs%20a%20mathematical%20operation%20on%20the%20data%20it%20receives%20before%20passing%20it%20to%20the%20next%20layer.%20To%20start%2C%20we%20will%20create%20a%20%22Hello%20World%22%20level%20model%20made%20from%204%20components%3A%0A%0A%20%20%20%201.%20A%20%5BFlatten%5D(https%3A%2F%2Fpytorch.org%2Fdocs%2Fstable%2Fgenerated%2Ftorch.nn.Flatten.html)%20used%20to%20convert%20n-dimensional%20data%20into%20a%20vector.%0A%20%20%20%202.%20An%20input%20layer%2C%20the%20first%20layer%20of%20neurons%0A%20%20%20%203.%20A%20hidden%20layer%2C%20another%20layer%20of%20neurons%20%22hidden%22%20between%20the%20input%20and%20output%0A%20%20%20%204.%20An%20output%20layer%2C%20the%20last%20set%20of%20neurons%20which%20returns%20the%20final%20prediction%20from%20the%20model%0A%0A%20%20%20%20We%20will%20use%20a%20variable%20called%20layers%20to%20store%20the%20layers%20of%20our%20model.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell(hide_code%3DTrue)%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%20Flatten%20the%20image%0A%0A%20%20%20%20The%20first%20thing%20we%20need%20to%20do%20is%20convert%20the%20image%20from%20a%2028x28%20%20array%20into%20a%20flat%201d%20tensor.%20We%20saw%20the%20images%20had%203%20dimensions%3A%20%60C%20x%20H%20x%20W%60.%20To%20flatten%20an%20image%20means%20to%20combine%20all%20of%20these%20images%20into%201%20dimension.%20Let's%20say%20we%20have%20a%20tensor%20like%20the%20one%20below.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(nn%2C%20torch)%3A%0A%20%20%20%20test_matrix%20%3D%20torch.tensor(%5B%5B1%2C%202%2C%203%5D%2C%20%5B4%2C%205%2C%206%5D%2C%20%5B7%2C%208%2C%209%5D%5D)%0A%20%20%20%20print(test_matrix)%0A%20%20%20%20print(nn.Flatten()(test_matrix))%0A%20%20%20%20return%20(test_matrix%2C)%0A%0A%0A%40app.cell(hide_code%3DTrue)%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20You%20will%20notice%20nothing%20happened%2C%20this%20is%20because%20neural%20networks%20expect%20to%20receive%20a%20batch%20of%20data.%20Currently%2C%20the%20Flatten%20layer%20sees%20three%20vectors%20as%20opposed%20to%20one%202d%20matrix.%20To%20fix%20this%2C%20we%20can%20%22batch%22%20our%20data%20by%20adding%20an%20extra%20pair%20of%20brackets.%20Since%20%60test_matrix%60%20is%20now%20a%20tensor%2C%20we%20can%20do%20that%20with%20the%20shorthand%20below.%20%60None%60%20adds%20a%20new%20dimension%20where%20%60%3A%60%20selects%20all%20the%20data%20in%20a%20tensor.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(nn%2C%20test_matrix)%3A%0A%20%20%20%20batch_test_matrix%20%3D%20test_matrix%5BNone%2C%20%3A%5D%0A%20%20%20%20print(batch_test_matrix)%0A%20%20%20%20print(nn.Flatten()(batch_test_matrix))%0A%20%20%20%20return%0A%0A%0A%40app.cell(hide_code%3DTrue)%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%20The%20Input%20Layer%0A%0A%20%20%20%20The%20input%20layer%20is%20the%20first%20layer%20of%20neurons%20in%20the%20neural%20network.%20It%20is%20responsible%20for%20receiving%20the%20input%20data%20and%20passing%20it%20to%20the%20next%20layer.%0A%0A%20%20%20%20This%20layer%20will%20be%20*densely%20connected*%2C%20meaning%20that%20each%20neuron%20in%20it%2C%20and%20its%20weights%2C%20will%20affect%20every%20neuron%20in%20the%20next%20layer.%0A%0A%20%20%20%20In%20order%20to%20create%20these%20weights%2C%20Pytorch%20needs%20to%20know%20the%20size%20of%20our%20inputs%20and%20how%20many%20neurons%20we%20want%20to%20create.%0A%20%20%20%20Since%20we've%20flattened%20our%20images%2C%20the%20size%20of%20our%20inputs%20is%20the%20number%20of%20channels%2C%20number%20of%20pixels%20vertically%2C%20and%20number%20of%20pixels%20horizontally%20multiplied%20together.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20input_size%20%3D%201%20*%2028%20*%2028%0A%20%20%20%20print(input_size)%0A%20%20%20%20return%20(input_size%2C)%0A%0A%0A%40app.cell(hide_code%3DTrue)%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20Choosing%20the%20correct%20number%20of%20neurons%20is%20what%20puts%20the%20%22science%22%20in%20%22data%20science%22%20as%20it%20is%20a%20matter%20of%20capturing%20the%20statistical%20complexity%20of%20the%20dataset.%20For%20now%2C%20we%20will%20use%20%60512%60%20neurons.%20Try%20playing%20around%20with%20this%20value%20later%20to%20see%20how%20it%20affects%20training%20and%20to%20start%20developing%20a%20sense%20for%20what%20this%20number%20means.%0A%0A%20%20%20%20We%20will%20learn%20more%20about%20activation%20functions%20later%2C%20but%20for%20now%2C%20we%20will%20use%20the%20%5Brelu%5D(https%3A%2F%2Fpytorch.org%2Fdocs%2Fstable%2Fgenerated%2Ftorch.nn.ReLU.html)%20activation%20function%2C%20which%20in%20short%2C%20will%20help%20our%20network%20to%20learn%20how%20to%20make%20more%20sophisticated%20guesses%20about%20data%20than%20if%20it%20were%20required%20to%20make%20guesses%20based%20on%20some%20strictly%20linear%20function.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(input_size%2C%20nn)%3A%0A%20%20%20%20layers_1%20%3D%20%5Bnn.Flatten()%2C%20nn.Linear(input_size%2C%20512)%2C%20nn.ReLU()%5D%0A%20%20%20%20layers_1%0A%20%20%20%20return%0A%0A%0A%40app.cell(hide_code%3DTrue)%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%20The%20hidden%20layer%0A%0A%20%20%20%20A%20hidden%20layer%20is%20a%20layer%20of%20neurons%20between%20the%20input%20and%20output%20layers.%20It%20is%20called%20%22hidden%22%20because%20it%20is%20not%20directly%20exposed%20to%20the%20input%20data%20and%20the%20output%20predictions.%20We%20will%20cover%20why%20we%20combine%20multiple%20layers%20in%20another%20lecture%2C%20but%20for%20now%2C%20we%20will%20add%20a%20hidden%20layer%20to%20our%20model.%0A%0A%20%20%20%20As%20with%20the%20previous%20layers%2C%20the%20shape%20of%20the%20data%20is%20important.%20%5Bnn.Linear%5D(https%3A%2F%2Fpytorch.org%2Fdocs%2Fstable%2Fgenerated%2Ftorch.nn.Linear.html)%20needs%20to%20know%20the%20shape%20of%20the%20data%20being%20passed%20to%20it.%20Each%20neuron%20in%20the%20previous%20layer%20will%20compute%20one%20number%2C%20so%20the%20number%20of%20inputs%20into%20the%20hidden%20layer%20is%20the%20same%20as%20the%20number%20of%20neurons%20in%20the%20previous%20later.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(input_size%2C%20nn)%3A%0A%20%20%20%20layers_2%20%3D%20%5B%0A%20%20%20%20%20%20%20%20nn.Flatten()%2C%0A%20%20%20%20%20%20%20%20nn.Linear(input_size%2C%20512)%2C%0A%20%20%20%20%20%20%20%20nn.ReLU()%2C%0A%20%20%20%20%20%20%20%20nn.Linear(512%2C%20512)%2C%0A%20%20%20%20%20%20%20%20nn.ReLU()%2C%0A%20%20%20%20%5D%0A%20%20%20%20layers_2%0A%20%20%20%20return%0A%0A%0A%40app.cell(hide_code%3DTrue)%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%20The%20Output%20Layer%0A%0A%20%20%20%20The%20output%20layer%20is%20the%20final%20layer%20of%20neurons%20in%20the%20neural%20network.%20It%20is%20responsible%20for%20producing%20the%20output%20of%20the%20model.%20This%20will%20be%20a%20tensor%20of%20length%2010%2C%20where%20each%20element%20represents%20the%20probability%20of%20the%20input%20image%20being%20a%20particular%20digit.%0A%0A%20%20%20%20We%20will%20not%20assign%20the%20%60relu%60%20function%20to%20the%20output%20layer.%20Instead%2C%20we%20will%20apply%20a%20%60loss%20function%60%20covered%20in%20the%20next%20section.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(input_size%2C%20nn)%3A%0A%20%20%20%20n_classes%20%3D%2010%0A%20%20%20%20layers_3%20%3D%20%5B%0A%20%20%20%20%20%20%20%20nn.Flatten()%2C%0A%20%20%20%20%20%20%20%20nn.Linear(input_size%2C%20512)%2C%0A%20%20%20%20%20%20%20%20nn.ReLU()%2C%0A%20%20%20%20%20%20%20%20nn.Linear(512%2C%20512)%2C%0A%20%20%20%20%20%20%20%20nn.ReLU()%2C%0A%20%20%20%20%20%20%20%20nn.Linear(512%2C%20n_classes)%2C%0A%20%20%20%20%5D%0A%20%20%20%20layers_3%0A%20%20%20%20return%20(layers_3%2C)%0A%0A%0A%40app.cell(hide_code%3DTrue)%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%20Compiling%20the%20Model%0A%0A%20%20%20%20A%20%5BSequential%5D(https%3A%2F%2Fpytorch.org%2Fdocs%2Fstable%2Fgenerated%2Ftorch.nn.Sequential.html)%20model%20expects%20a%20sequence%20of%20arguments%2C%20not%20a%20list%2C%20so%20we%20can%20use%20the%20%5B*%20operator%5D(https%3A%2F%2Fdocs.python.org%2F3%2Freference%2Fexpressions.html%23expression-lists)%20to%20unpack%20our%20list%20of%20layers%20into%20a%20sequence.%20We%20can%20print%20the%20model%20to%20verify%20these%20layers%20loaded%20correctly.%20We%20can%20also%20send%20our%20model%20to%20the%20GPU%20using%20the%20to(device)%20method.%20We%20can%20verify%20where%20the%20model%20is%20by%20using%20the%20.device%20attribute.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(device%2C%20layers_3%2C%20nn)%3A%0A%20%20%20%20model%20%3D%20nn.Sequential(*layers_3)%0A%20%20%20%20model.to(device)%0A%20%20%20%20next(model.parameters()).device%0A%20%20%20%20return%20(model%2C)%0A%0A%0A%40app.cell(hide_code%3DTrue)%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20PyTorch%202.0%20introduced%20the%20ability%20to%20%5Bcompile%5D(%5Bhere%5D(https%3A%2F%2Fpytorch.org%2Ftutorials%2Fintermediate%2Ftorch_compile_tutorial.html).)%20the%20model%20using%20the%20%60compile%60%20method%20which%20can%20give%20faster%20performance.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(model%2C%20torch)%3A%0A%20%20%20%20model_compiled%20%3D%20torch.compile(model)%0A%20%20%20%20return%20(model_compiled%2C)%0A%0A%0A%40app.cell(hide_code%3DTrue)%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%20Training%20the%20Model%0A%0A%20%20%20%20Now%20that%20we%20have%20prepared%20training%20and%20validation%20data%2C%20and%20a%20model%2C%20it's%20time%20to%20train%20our%20model%20with%20our%20training%20data%2C%20and%20verify%20it%20with%20its%20validation%20data.%0A%0A%20%20%20%20This%20is%20called%20fitting%20and%20we%20need%20to%20add%20two%20functions%20to%20help%20us%20with%20this%20process.%0A%0A%20%20%20%20%23%23%20Loss%20and%20Optimization%0A%0A%20%20%20%20The%20loss%20function%20measures%20the%20difference%20between%20the%20model's%20prediction%20and%20the%20target.%20The%20optimizer%20updates%20the%20model's%20parameters%20to%20reduce%20the%20loss.%20In%20this%20example%20we%20will%20use%20%20a%20loss%20function%20called%20%5BCrossEntropy%5D(https%3A%2F%2Fpytorch.org%2Fdocs%2Fstable%2Fgenerated%2Ftorch.nn.CrossEntropyLoss.html)%20which%20is%20designed%20to%20grade%20if%20a%20model%20predicted%20the%20correct%20category%20from%20a%20group%20of%20categories.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(nn)%3A%0A%20%20%20%20loss_function%20%3D%20nn.CrossEntropyLoss()%0A%20%20%20%20return%20(loss_function%2C)%0A%0A%0A%40app.cell(hide_code%3DTrue)%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20The%20optimizer%20is%20used%20to%20update%20the%20model's%20weights%20based%20on%20the%20data%20it%20sees%20and%20the%20loss%20function.%20We%20will%20use%20the%20%5BAdam%5D(https%3A%2F%2Fpytorch.org%2Fdocs%2Fstable%2Foptim.html)%20optimizer%2C%20which%20is%20a%20popular%20optimizer%20in%20deep%20learning.%20It%20needs%20to%20know%20the%20models%20parameters%20so%20it%20can%20update%20them.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(Adam%2C%20model_compiled)%3A%0A%20%20%20%20optimizer%20%3D%20Adam(model_compiled.parameters())%0A%20%20%20%20return%20(optimizer%2C)%0A%0A%0A%40app.cell(hide_code%3DTrue)%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%20Accuracy%0A%0A%20%20%20%20When%20training%20a%20model%2C%20it%20is%20important%20to%20know%20how%20well%20it%20is%20performing.%20We%20can%20calculate%20the%20accuracy%20of%20the%20model%20by%20comparing%20the%20model's%20prediction%20to%20the%20actual%20target.%20The%20simplest%20way%20to%20do%20this%20is%20to%20compare%20the%20model's%20prediction%20to%20the%20target%20and%20calculate%20the%20percentage%20of%20correct%20predictions.%0A%0A%20%20%20%20We%20need%20to%20generate%20our%20own%20accuracy%20functions%20as%20these%20are%20typically%20dependent%20on%20the%20problem%20being%20solved.%0A%0A%20%20%20%20We%20need%20to%20compare%20the%20number%20of%20correct%20classifications%20compared%20to%20the%20total%20number%20of%20predictions%20made.%20Since%20we're%20showing%20data%20to%20the%20model%20in%20batches%2C%20our%20accuracy%20can%20be%20calculated%20along%20with%20these%20batches.%0A%0A%20%20%20%20We%20typically%20use%20N%20as%20a%20postfix%20to%20denote%20the%20number%20of%20samples%20in%20a%20dataset.%20We%20can%20use%20this%20to%20calculate%20the%20accuracy%20of%20our%20model.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(train_loader%2C%20valid_loader)%3A%0A%20%20%20%20train_N%20%3D%20len(train_loader.dataset)%0A%20%20%20%20valid_N%20%3D%20len(valid_loader.dataset)%0A%20%20%20%20return%20train_N%2C%20valid_N%0A%0A%0A%40app.cell(hide_code%3DTrue)%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20We%20can%20then%20accumulate%20the%20accuracy%20for%20each%20batch%20and%20divide%20by%20the%20total%20number%20of%20samples%20to%20get%20the%20overall%20accuracy.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.function%0Adef%20get_batch_accuracy(output%2C%20y%2C%20N)%3A%0A%20%20%20%20pred%20%3D%20output.argmax(dim%3D1%2C%20keepdim%3DTrue)%0A%20%20%20%20correct%20%3D%20pred.eq(y.view_as(pred)).sum().item()%0A%20%20%20%20return%20correct%20%2F%20N%0A%0A%0A%40app.cell(hide_code%3DTrue)%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%20The%20Training%20Loop%0A%0A%20%20%20%20We%20now%20generate%20a%20function%20that%20will%20train%20the%20model%20for%20a%20single%20epoch.%20This%20is%20similar%20to%20the%20approch%20we%20used%20in%20the%20the%20Linear%20example%20but%20we%20have%20added%20a%20few%20more%20steps%20to%20calculate%20the%20accuracy%20of%20the%20model.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(device%2C%20loss_function%2C%20model_compiled%2C%20optimizer%2C%20train_N%2C%20train_loader)%3A%0A%20%20%20%20def%20train()%3A%0A%20%20%20%20%20%20%20%20loss%20%3D%200%0A%20%20%20%20%20%20%20%20accuracy%20%3D%200%0A%20%20%20%20%20%20%20%20model_compiled.train()%0A%20%20%20%20%20%20%20%20for%20x%2C%20y%20in%20train_loader%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20x%2C%20y%20%3D%20(x.to(device)%2C%20y.to(device))%0A%20%20%20%20%20%20%20%20%20%20%20%20output%20%3D%20model_compiled(x)%0A%20%20%20%20%20%20%20%20%20%20%20%20optimizer.zero_grad()%0A%20%20%20%20%20%20%20%20%20%20%20%20batch_loss%20%3D%20loss_function(output%2C%20y)%0A%20%20%20%20%20%20%20%20%20%20%20%20batch_loss.backward()%0A%20%20%20%20%20%20%20%20%20%20%20%20optimizer.step()%0A%20%20%20%20%20%20%20%20%20%20%20%20loss%20%3D%20loss%20%2B%20batch_loss.item()%0A%20%20%20%20%20%20%20%20%20%20%20%20accuracy%20%3D%20accuracy%20%2B%20get_batch_accuracy(output%2C%20y%2C%20train_N)%0A%20%20%20%20%20%20%20%20print(%22Train%20-%20Loss%3A%20%7B%3A.4f%7D%20Accuracy%3A%20%7B%3A.4f%7D%22.format(loss%2C%20accuracy))%0A%20%20%20%20return%20(train%2C)%0A%0A%0A%40app.cell(hide_code%3DTrue)%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%20Validation%0A%0A%20%20%20%20Once%20the%20model%20has%20done%20a%20training%20step%20we%20need%20to%20see%20how%20close%20we%20are%20to%20the%20correct%20answer.%20We%20can%20do%20this%20by%20running%20the%20model%20on%20the%20validation%20data%20and%20calculating%20the%20loss%20and%20accuracy%20of%20the%20model%20on%20the%20validation%20data.%20Agin%20we%20will%20use%20a%20function%20to%20do%20this.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(device%2C%20loss_function%2C%20model_compiled%2C%20torch%2C%20valid_N%2C%20valid_loader)%3A%0A%20%20%20%20def%20validate()%3A%0A%20%20%20%20%20%20%20%20loss%20%3D%200%0A%20%20%20%20%20%20%20%20accuracy%20%3D%200%0A%20%20%20%20%20%20%20%20model_compiled.eval()%0A%20%20%20%20%20%20%20%20with%20torch.no_grad()%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20for%20x%2C%20y%20in%20valid_loader%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20x%2C%20y%20%3D%20(x.to(device)%2C%20y.to(device))%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20output%20%3D%20model_compiled(x)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20loss%20%3D%20loss%20%2B%20loss_function(output%2C%20y).item()%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20accuracy%20%3D%20accuracy%20%2B%20get_batch_accuracy(output%2C%20y%2C%20valid_N)%0A%20%20%20%20%20%20%20%20print(%22Valid%20-%20Loss%3A%20%7B%3A.4f%7D%20Accuracy%3A%20%7B%3A.4f%7D%22.format(loss%2C%20accuracy))%0A%20%20%20%20return%20(validate%2C)%0A%0A%0A%40app.cell(hide_code%3DTrue)%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%20The%20Training%20loop%0A%0A%20%20%20%20We%20can%20now%20create%20a%20training%20loop%20that%20will%20train%20the%20model%20for%20a%20number%20of%20epochs.%20An%20%60epoch%60%20is%20one%20complete%20pass%20through%20the%20entire%20dataset.%20Let's%20train%20and%20validate%20the%20model%20for%205%20%60epochs%60%20to%20see%20how%20it%20learns.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(train%2C%20validate)%3A%0A%20%20%20%20epochs%20%3D%2010%0A%0A%20%20%20%20for%20epoch%20in%20range(epochs)%3A%0A%20%20%20%20%20%20%20%20print(%22Epoch%3A%20%7B%7D%22.format(epoch))%0A%20%20%20%20%20%20%20%20train()%0A%20%20%20%20%20%20%20%20validate()%0A%20%20%20%20return%0A%0A%0A%40app.cell(hide_code%3DTrue)%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20We%20can%20see%20that%20we%20are%20quite%20close%20(nearly%20100%25)%20so%20we%20can%20try%20and%20test%20our%20model%20on%20some%20existing%20data.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(device%2C%20model_compiled%2C%20test_images_tensor)%3A%0A%20%20%20%20prediction%20%3D%20model_compiled(test_images_tensor%5B0%5D.to(device).unsqueeze(0))%0A%20%20%20%20prediction%0A%20%20%20%20return%20(prediction%2C)%0A%0A%0A%40app.cell(hide_code%3DTrue)%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20There%20should%20be%20ten%20numbers%2C%20each%20corresponding%20to%20a%20different%20output%20neuron.%20Thanks%20to%20how%20the%20data%20is%20structured%2C%20the%20index%20of%20each%20number%20matches%20the%20corresponding%20handwritten%20number.%20The%200th%20index%20is%20a%20prediction%20for%20a%20handwritten%200%2C%20the%201st%20index%20is%20a%20prediction%20for%20a%20handwritten%201%2C%20and%20so%20on.%0A%0A%20%20%20%20We%20can%20use%20the%20%60argmax%60%20function%20to%20find%20the%20index%20of%20the%20highest%20value.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(display_image%2C%20prediction%2C%20test_images)%3A%0A%20%20%20%20print(prediction.argmax(dim%3D1%2C%20keepdim%3DTrue))%0A%20%20%20%20display_image(test_images%5B0%5D%2C%20prediction.argmax(dim%3D1%2C%20keepdim%3DTrue).item())%0A%20%20%20%20return%0A%0A%0A%40app.cell(hide_code%3DTrue)%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20This%20seems%20to%20have%20worked%2C%20we%20should%20now%20save%20the%20model%20so%20we%20can%20use%20it%20later.%0A%0A%20%20%20%20%23%23%20Saving%20the%20Model%0A%0A%20%20%20%20We%20can%20save%20the%20model%20using%20the%20%60%60%60torch.save%60%60%60%20function.%20We%20can%20save%20the%20model%20to%20a%20file%20called%20%60digits.pth%60%20in%20the%20current%20directory.%20Note%20we%20save%20the%20original%20uncompiled%20model%20for%20simplicity%20as%20when%20compiled%20names%20get%20changed.%20We%20can%20always%20re-compile%20later%20if%20required.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(%0A%20%20%20%20device%2C%0A%20%20%20%20display_image%2C%0A%20%20%20%20layers_3%2C%0A%20%20%20%20model%2C%0A%20%20%20%20nn%2C%0A%20%20%20%20test_images%2C%0A%20%20%20%20test_images_tensor%2C%0A%20%20%20%20torch%2C%0A)%3A%0A%20%20%20%20torch.save(model.state_dict()%2C%20%22mnist_model.pth%22)%0A%20%20%20%20model2%20%3D%20nn.Sequential(*layers_3)%0A%20%20%20%20model2.load_state_dict(torch.load(%22mnist_model.pth%22))%0A%20%20%20%20model2.to(device)%0A%20%20%20%20model2.eval()%0A%20%20%20%20prediction_1%20%3D%20model2(test_images_tensor%5B0%5D.to(device).unsqueeze(0))%0A%20%20%20%20torch.save(test_images_tensor%5B0%5D%2C%20%22test_image.pth%22)%0A%20%20%20%20display_image(test_images%5B0%5D%2C%20prediction_1.argmax(dim%3D1%2C%20keepdim%3DTrue).item())%0A%20%20%20%20return%0A%0A%0A%40app.cell(hide_code%3DTrue)%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20In%20the%20above%20case%20we%20saved%20only%20the%20model%20state%20dictionary.%20We%20can%20also%20save%20the%20entire%20model%20including%20the%20architecture%20and%20the%20state%20dictionary.%20This%20is%20a%20little%20more%20rigid%20as%20it%20requires%20the%20model%20to%20be%20defined%20in%20the%20same%20way%20when%20it%20is%20loaded%2C%20however%20we%20don't%20also%20need%20to%20re-create%20the%20model%20architecture%20when%20we%20load%20the%20model.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(device%2C%20display_image%2C%20model%2C%20test_images%2C%20test_images_tensor%2C%20torch)%3A%0A%20%20%20%20torch.save(model%2C%20%22minst_model_full.pth%22)%0A%20%20%20%20model3%20%3D%20torch.load(%22minst_model_full.pth%22%2C%20weights_only%3DFalse)%0A%20%20%20%20model3.to(device)%0A%20%20%20%20model3.eval()%0A%20%20%20%20prediction_2%20%3D%20model3(test_images_tensor%5B0%5D.to(device).unsqueeze(0))%0A%20%20%20%20display_image(test_images%5B0%5D%2C%20prediction_2.argmax(dim%3D1%2C%20keepdim%3DTrue).item())%0A%20%20%20%20return%0A%0A%0A%40app.cell(hide_code%3DTrue)%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20We%20can%20now%20use%20this%20model%20to%20classify%20new%20images%20of%20digits.%20We%20will%20demonstrate%20this%20in%20a%20stand%20alone%20Qt%20applications.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell(hide_code%3DTrue)%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%20Conclusion%0A%0A%20%20%20%20In%20this%20notebook%20we%20have%20trained%20a%20neural%20network%20to%20recognize%20handwritten%20digits.%20We%20have%20used%20the%20MNIST%20dataset%20to%20train%20the%20model%20and%20have%20used%20a%20simple%20neural%20network%20with%20a%20single%20hidden%20layer%20to%20classify%20the%20images.%20We%20have%20trained%20the%20model%20for%205%20epochs%20and%20have%20achieved%20an%20accuracy%20of%20nearly%20100%25.%20We%20have%20saved%20the%20model%20so%20we%20can%20use%20it%20later.%0A%0A%20%20%20%20This%20is%20basically%20the%20process%20we%20will%20use%20for%20all%20of%20our%20machine%20learning%20models.%20We%20will%20load%20the%20data%2C%20create%20a%20model%2C%20train%20the%20model%20and%20then%20save%20the%20model.%20We%20can%20then%20use%20the%20model%20to%20classify%20new%20data.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20import%20marimo%20as%20mo%0A%20%20%20%20return%20(mo%2C)%0A%0A%0Aif%20__name__%20%3D%3D%20%22__main__%22%3A%0A%20%20%20%20app.run()%0A
</marimo-code>

<marimo-code-hash hidden="">2ce79fd60df57b27e05b54243cec7619</marimo-code-hash>
</body>
</html>

